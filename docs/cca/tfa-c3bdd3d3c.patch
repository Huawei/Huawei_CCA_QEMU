diff --git a/.gitignore b/.gitignore
index b005fab38..d5d62ec7b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -5,6 +5,7 @@ cscope.*
 *~
 .project
 .cproject
+.vscode
 
 # Ignore build directory
 build/
diff --git a/Makefile b/Makefile
index 16c85bcd2..b675027e2 100644
--- a/Makefile
+++ b/Makefile
@@ -147,7 +147,7 @@ endif
 CTX_INCLUDE_EL2_REGS := 1
 CTX_INCLUDE_AARCH32_REGS := 0
 ARM_ARCH_MAJOR := 8
-ARM_ARCH_MINOR := 6
+ARM_ARCH_MINOR := 2
 endif
 
 # USE_SPINLOCK_CAS requires AArch64 build
@@ -1077,6 +1077,7 @@ $(eval $(call assert_numerics,\
         ENABLE_FEAT_VHE \
         ENABLE_MPAM_FOR_LOWER_ELS \
         ENABLE_RME \
+        RME_DEBUG \
         ENABLE_TRF_FOR_NS \
         FW_ENC_STATUS \
         NR_OF_FW_BANKS \
@@ -1128,6 +1129,7 @@ $(eval $(call add_defines,\
         ENABLE_PMF \
         ENABLE_PSCI_STAT \
         ENABLE_RME \
+        RME_DEBUG \
         ENABLE_RUNTIME_INSTRUMENTATION \
         ENABLE_SME_FOR_NS \
         ENABLE_SME_FOR_SWD \
@@ -1204,6 +1206,7 @@ $(eval $(call add_defines,\
         FEATURE_DETECTION \
         TWED_DELAY \
         ENABLE_FEAT_TWED \
+        ENABLE_SMMU \
 )))
 
 ifeq (${SANITIZE_UB},trap)
diff --git a/include/arch/aarch64/arch.h b/include/arch/aarch64/arch.h
index bbbc77adb..886c9ae4e 100644
--- a/include/arch/aarch64/arch.h
+++ b/include/arch/aarch64/arch.h
@@ -96,6 +96,34 @@
 #define ICC_EOIR1_EL1		S3_0_c12_c12_1
 #define ICC_SGI0R_EL1		S3_0_c12_c11_7
 
+#define ICH_VTR_EL2 S3_4_C12_C11_1
+#define ICH_MISR_EL2 S3_4_C12_C11_2
+#define ICH_ELRSR_EL2 S3_4_C12_C11_5
+#define ICH_LR0_EL2 S3_4_C12_C12_0
+#define ICH_LR1_EL2 S3_4_C12_C12_1
+#define ICH_LR2_EL2 S3_4_C12_C12_2
+#define ICH_LR3_EL2 S3_4_C12_C12_3
+#define ICH_LR4_EL2 S3_4_C12_C12_4
+#define ICH_LR5_EL2 S3_4_C12_C12_5
+#define ICH_LR6_EL2 S3_4_C12_C12_6
+#define ICH_LR7_EL2 S3_4_C12_C12_7
+#define ICH_LR8_EL2 S3_4_C12_C13_0
+#define ICH_LR9_EL2 S3_4_C12_C13_1
+#define ICH_LR10_EL2 S3_4_C12_C13_2
+#define ICH_LR11_EL2 S3_4_C12_C13_3
+#define ICH_LR12_EL2 S3_4_C12_C13_4
+#define ICH_LR13_EL2 S3_4_C12_C13_5
+#define ICH_LR14_EL2 S3_4_C12_C13_6
+#define ICH_LR15_EL2 S3_4_C12_C13_7
+#define ICH_AP0R0_EL2 S3_4_C12_C8_0
+#define ICH_AP0R1_EL2 S3_4_C12_C8_1
+#define ICH_AP0R2_EL2 S3_4_C12_C8_2
+#define ICH_AP0R3_EL2 S3_4_C12_C8_3
+#define ICH_AP1R0_EL2 S3_4_C12_C9_0
+#define ICH_AP1R1_EL2 S3_4_C12_C9_1
+#define ICH_AP1R2_EL2 S3_4_C12_C9_2
+#define ICH_AP1R3_EL2 S3_4_C12_C9_3
+
 /*******************************************************************************
  * Definitions for EL2 system registers for save/restore routine
  ******************************************************************************/
@@ -121,6 +149,17 @@
 #define PMSCR_EL2		S3_4_C9_C9_0
 #define TFSR_EL2		S3_4_C5_C6_0
 
+/*******************************************************************************
+ * GIC registers bit definitions
+ ******************************************************************************/
+#define ICC_SRE_EL2_SRE     (U(1) << 0)
+#define ICC_SRE_EL2_DFB		(U(1) << 1)
+#define ICC_SRE_EL2_DIB		(U(1) << 2)
+#define ICC_SRE_EL2_ENABLE  (U(1) << 3)
+#define ICC_SRE_EL1_SRE     (U(1) << 0)
+#define ICC_SRE_EL1_DFB		(U(1) << 1)
+#define ICC_SRE_EL1_DIB		(U(1) << 2)
+
 /*******************************************************************************
  * Generic timer memory mapped registers & offsets
  ******************************************************************************/
@@ -568,6 +607,8 @@
 
 /* CNTHP_CTL_EL2 definitions */
 #define CNTHP_CTL_ENABLE_BIT	(U(1) << 0)
+#define CNTHP_CTL_IT_MASK	(U(1) << 1)
+#define CNTHP_CTL_IT_STAT	(U(1) << 2)
 #define CNTHP_CTL_RESET_VAL	U(0x0)
 
 /* VTTBR_EL2 definitions */
@@ -582,18 +623,31 @@
 #define HCR_AMVOFFEN_SHIFT	U(51)
 #define HCR_AMVOFFEN_BIT	(ULL(1) << HCR_AMVOFFEN_SHIFT)
 #define HCR_TEA_BIT		(ULL(1) << 47)
+#define HCR_FWB_BIT		(ULL(1) << 46)
 #define HCR_API_BIT		(ULL(1) << 41)
 #define HCR_APK_BIT		(ULL(1) << 40)
 #define HCR_E2H_BIT		(ULL(1) << 34)
 #define HCR_HCD_BIT		(ULL(1) << 29)
 #define HCR_TGE_BIT		(ULL(1) << 27)
+#define HCR_TSC_BIT		(ULL(1) << 19)
 #define HCR_RW_SHIFT		U(31)
 #define HCR_RW_BIT		(ULL(1) << HCR_RW_SHIFT)
 #define HCR_TWE_BIT		(ULL(1) << 14)
 #define HCR_TWI_BIT		(ULL(1) << 13)
+#define HCR_BSU_SHIFT	(U(10))
+#define HCR_BSU_RESET_VAL	(ULL(0x0) << HCR_BSU_SHIFT)
+#define HCR_BSU_IS_VAL		(ULL(0x1) << HCR_BSU_SHIFT)
+#define HCR_BSU_OS_VAL		(ULL(0x2) << HCR_BSU_SHIFT)
+#define HCR_BSU_FULL_VAL	(ULL(0x3) << HCR_BSU_SHIFT)
+#define HCR_FB_BIT      (ULL(1) << 9)
+#define HCR_VI_BIT		(ULL(1) << 7)
+#define HCR_VF_BIT		(ULL(1) << 6)
 #define HCR_AMO_BIT		(ULL(1) << 5)
 #define HCR_IMO_BIT		(ULL(1) << 4)
 #define HCR_FMO_BIT		(ULL(1) << 3)
+#define HCR_PTW_BIT		(ULL(1) << 2)
+#define HCR_SWIO_BIT	(ULL(1) << 1)
+#define HCR_VM_BIT		(ULL(1) << 0)
 
 /* ISR definitions */
 #define ISR_A_SHIFT		U(8)
@@ -643,6 +697,51 @@
 /* VTCR_EL2 definitions */
 #define VTCR_RESET_VAL		U(0x0)
 #define VTCR_EL2_MSA		(U(1) << 31)
+#define VTCR_RES1_B31_EL2	(U(1) << 31)
+
+#define VTCR_PS_SHIFT_EL2 16
+#define VTCR_PS_32BIT_EL2 (U(0) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_36BIT_EL2 (U(1) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_40BIT_EL2 (U(2) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_42BIT_EL2 (U(3) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_44BIT_EL2 (U(4) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_48BIT_EL2 (U(5) << VTCR_PS_SHIFT_EL2)
+#define VTCR_PS_52BIT_EL2 (U(6) << VTCR_PS_SHIFT_EL2)
+
+#define VTCR_TG0_SHIFT_EL2 14
+#define VTCR_TG0_4K_EL2		(U(0) << VTCR_TG0_SHIFT_EL2)
+#define VTCR_TG0_16K_EL2	(U(2) << VTCR_TG0_SHIFT_EL2)
+#define VTCR_TG0_64K_EL2	(U(1) << VTCR_TG0_SHIFT_EL2)
+
+#define VTCR_SH0_SHIFT_EL2 12
+#define VTCR_SH0_IS_EL2		(U(3) << VTCR_SH0_SHIFT_EL2)
+#define VTCR_SH0_OS_EL2		(U(2) << VTCR_SH0_SHIFT_EL2)
+#define VTCR_SH0_NS_EL2		(U(0) << VTCR_SH0_SHIFT_EL2)
+
+#define VTCR_ORGN0_SHIFT_EL2 10
+#define VTCR_ORGN0_NC_EL2		(U(0) << VTCR_ORGN0_SHIFT_EL2)
+#define VTCR_ORGN0_WBMA_EL2		(U(1) << VTCR_ORGN0_SHIFT_EL2)
+#define VTCR_ORGN0_WTNWA_EL2	(U(2) << VTCR_ORGN0_SHIFT_EL2)
+#define VTCR_ORGN0_WBNWA_EL2	(U(3) << VTCR_ORGN0_SHIFT_EL2)
+
+#define VTCR_IRGN0_SHIFT_EL2 8
+#define VTCR_IRGN0_NC_EL2		(U(0) << VTCR_IRGN0_SHIFT_EL2)
+#define VTCR_IRGN0_WBMA_EL2		(U(1) << VTCR_IRGN0_SHIFT_EL2)
+#define VTCR_IRGN0_WTNWA_EL2	(U(2) << VTCR_IRGN0_SHIFT_EL2)
+#define VTCR_IRGN0_WBNWA_EL2	(U(3) << VTCR_IRGN0_SHIFT_EL2)
+
+#define VTCR_SL0_SHIFT_EL2 6
+#define VTCR_SL0_EL2(x) 	((2UL - (uint64_t)(x)) << VTCR_SL0_SHIFT_EL2)
+#define VTCR_SL0_LEVEL0_EL2	VTCR_SL0_EL2(2)
+#define VTCR_SL0_LEVEL1_EL2	VTCR_SL0_EL2(1)
+#define VTCR_SL0_LEVEL2_EL2	VTCR_SL0_EL2(0)
+
+#define VTCR_T0SZ_EL2(x)	(((uint64_t)(64UL - (uint64_t)(x))))
+
+#define VTCR_INIT_FLAGS_EL2 (VTCR_TG0_4K_EL2 | VTCR_SH0_IS_EL2 | \
+							 VTCR_IRGN0_WBMA_EL2 | VTCR_ORGN0_WBMA_EL2 | \
+							 VTCR_RES1_B31_EL2)
+
 
 /* CPSR/SPSR definitions */
 #define DAIF_FIQ_BIT		(U(1) << 0)
diff --git a/include/arch/aarch64/arch_helpers.h b/include/arch/aarch64/arch_helpers.h
index 10b0a0b97..2e84e983f 100644
--- a/include/arch/aarch64/arch_helpers.h
+++ b/include/arch/aarch64/arch_helpers.h
@@ -400,6 +400,8 @@ DEFINE_SYSREG_RW_FUNCS(far_el1)
 DEFINE_SYSREG_RW_FUNCS(far_el2)
 DEFINE_SYSREG_RW_FUNCS(far_el3)
 
+DEFINE_SYSREG_RW_FUNCS(hpfar_el2)
+
 DEFINE_SYSREG_RW_FUNCS(mair_el1)
 DEFINE_SYSREG_RW_FUNCS(mair_el2)
 DEFINE_SYSREG_RW_FUNCS(mair_el3)
@@ -444,6 +446,7 @@ DEFINE_SYSREG_RW_FUNCS(cntp_tval_el0)
 DEFINE_SYSREG_RW_FUNCS(cntp_cval_el0)
 DEFINE_SYSREG_READ_FUNC(cntpct_el0)
 DEFINE_SYSREG_RW_FUNCS(cnthctl_el2)
+DEFINE_SYSREG_RW_FUNCS(cntv_ctl_el0)
 
 DEFINE_SYSREG_RW_FUNCS(vtcr_el2)
 
@@ -493,6 +496,36 @@ DEFINE_RENAME_SYSREG_WRITE_FUNC(icc_eoir1_el1, ICC_EOIR1_EL1)
 DEFINE_RENAME_SYSREG_WRITE_FUNC(icc_sgi0r_el1, ICC_SGI0R_EL1)
 DEFINE_RENAME_SYSREG_RW_FUNCS(icc_sgi1r, ICC_SGI1R)
 
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_vmcr_el2, ICH_VMCR_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_hcr_el2, ICH_HCR_EL2)
+DEFINE_RENAME_SYSREG_READ_FUNC(ich_vtr_el2, ICH_VTR_EL2)
+DEFINE_RENAME_SYSREG_READ_FUNC(ich_misr_el2, ICH_MISR_EL2)
+DEFINE_RENAME_SYSREG_READ_FUNC(ich_elrsr_el2, ICH_ELRSR_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr0_el2, ICH_LR0_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr1_el2, ICH_LR1_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr2_el2, ICH_LR2_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr3_el2, ICH_LR3_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr4_el2, ICH_LR4_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr5_el2, ICH_LR5_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr6_el2, ICH_LR6_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr7_el2, ICH_LR7_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr8_el2, ICH_LR8_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr9_el2, ICH_LR9_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr10_el2, ICH_LR10_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr11_el2, ICH_LR11_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr12_el2, ICH_LR12_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr13_el2, ICH_LR13_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr14_el2, ICH_LR14_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_lr15_el2, ICH_LR15_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap0r0_el2, ICH_AP0R0_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap0r1_el2, ICH_AP0R1_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap0r2_el2, ICH_AP0R2_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap0r3_el2, ICH_AP0R3_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap1r0_el2, ICH_AP1R0_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap1r1_el2, ICH_AP1R1_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap1r2_el2, ICH_AP1R2_EL2)
+DEFINE_RENAME_SYSREG_RW_FUNCS(ich_ap1r3_el2, ICH_AP1R3_EL2)
+
 DEFINE_RENAME_SYSREG_READ_FUNC(amcfgr_el0, AMCFGR_EL0)
 DEFINE_RENAME_SYSREG_READ_FUNC(amcgcr_el0, AMCGCR_EL0)
 DEFINE_RENAME_SYSREG_READ_FUNC(amcg1idr_el0, AMCG1IDR_EL0)
diff --git a/include/lib/xlat_tables/xlat_tables_v2.h b/include/lib/xlat_tables/xlat_tables_v2.h
index 69ad02764..c05cc089f 100644
--- a/include/lib/xlat_tables/xlat_tables_v2.h
+++ b/include/lib/xlat_tables/xlat_tables_v2.h
@@ -292,6 +292,17 @@ void mmap_add_region_alloc_va_ctx(xlat_ctx_t *ctx, mmap_region_t *mm);
 void mmap_add_alloc_va(mmap_region_t *mm);
 
 #if PLAT_XLAT_TABLES_DYNAMIC
+/*
+ * Remap a page with changed attributes, mainly for RMM to access NS memory.
+ */
+void rmm_remap_xlat_table_entry(unsigned long long va_addr, unsigned int attr);
+void rmm_undo_remap_xlat_table_entry(unsigned long long va_addr);
+void remap_l2_block_entry(xlat_ctx_t *ctx, uintptr_t table_base_va,
+		uint64_t va_addr, unsigned int attr);
+void undo_remap_l2_block_entry(xlat_ctx_t *ctx, uintptr_t table_base_va,
+		uint64_t va_addr);
+
+
 /*
  * Add a dynamic region with defined base PA and base VA. This type of region
  * can be added and removed even after the translation tables are initialized.
diff --git a/include/plat/arm/common/arm_def.h b/include/plat/arm/common/arm_def.h
index a8211bdc6..346e19332 100644
--- a/include/plat/arm/common/arm_def.h
+++ b/include/plat/arm/common/arm_def.h
@@ -295,6 +295,16 @@
 					PLAT_ARM_RMM_SIZE,		\
 					MT_MEMORY | MT_RW | MT_REALM)
 
+#define ARM_MAP_RMM_CODE MAP_REGION_FLAT( 				\
+					PLAT_ARM_RMM_CODE_BASE,		\
+					PLAT_ARM_RMM_CODE_SIZE,		\
+					MT_CODE | MT_REALM)
+
+#define ARM_MAP_RMM_RODATA MAP_REGION_FLAT( \
+					PLAT_ARM_RMM_RODATA_BASE,	\
+					PLAT_ARM_RMM_RODATA_SIZE,	\
+					MT_RO_DATA | MT_REALM)
+
 
 #define ARM_MAP_GPT_L1_DRAM	MAP_REGION_FLAT(			\
 					ARM_L1_GPT_ADDR_BASE,		\
diff --git a/include/services/trp/platform_trp.h b/include/services/trp/platform_trp.h
index b34da8512..1d79c7f99 100644
--- a/include/services/trp/platform_trp.h
+++ b/include/services/trp/platform_trp.h
@@ -11,5 +11,7 @@
  * Mandatory TRP functions (only if platform contains a TRP)
  ******************************************************************************/
 void trp_early_platform_setup(void);
+void trp_plat_arch_setup(void);
+void trp_plat_arch_enable_mmu(int linearId);
 
 #endif /* PLATFORM_TRP_H */
diff --git a/lib/gpt_rme/gpt_rme.c b/lib/gpt_rme/gpt_rme.c
index d6fbc04b9..a26c2be83 100644
--- a/lib/gpt_rme/gpt_rme.c
+++ b/lib/gpt_rme/gpt_rme.c
@@ -697,6 +697,12 @@ int gpt_enable(void)
 	tlbipaallos();
 	dsb();
 
+	/*
+	 * SCR_EL3.GPF == 0, GPF at EL0/1/2 is reported synchronously as
+	 * Instruction Abort or Data Abort exception.
+	 */
+	write_scr(read_scr() & ~SCR_GPF_BIT);
+
 	/* Write the base address of the L0 tables into GPTBR */
 	write_gptbr_el3(((gpt_config.plat_gpt_l0_base >> GPTBR_BADDR_VAL_SHIFT)
 			>> GPTBR_BADDR_SHIFT) & GPTBR_BADDR_MASK);
@@ -719,9 +725,8 @@ int gpt_enable(void)
 	gpccr_el3 |= SET_GPCCR_IRGN(GPCCR_IRGN_WB_RA_WA);
 
 	/* Enable GPT */
+	VERBOSE("Enabling GPCCR_EL3\n");
 	gpccr_el3 |= GPCCR_GPC_BIT;
-
-	/* TODO: Configure GPCCR_EL3_GPCP for Fault control. */
 	write_gpccr_el3(gpccr_el3);
 	isb();
 	tlbipaallos();
diff --git a/lib/xlat_tables_v2/aarch64/xlat_tables_arch.c b/lib/xlat_tables_v2/aarch64/xlat_tables_arch.c
index 719110a0e..3edb7182e 100644
--- a/lib/xlat_tables_v2/aarch64/xlat_tables_arch.c
+++ b/lib/xlat_tables_v2/aarch64/xlat_tables_arch.c
@@ -66,15 +66,20 @@ uint32_t xlat_arch_get_pas(uint32_t attr)
 
 	switch (pas) {
 #if ENABLE_RME
+#ifndef IMAGE_RMM
 	/* TTD.NSE = 1 and TTD.NS = 1 for Realm PAS */
 	case MT_REALM:
 		return LOWER_ATTRS(EL3_S1_NSE | NS);
 	/* TTD.NSE = 1 and TTD.NS = 0 for Root PAS */
 	case MT_ROOT:
 		return LOWER_ATTRS(EL3_S1_NSE);
+#endif
 #endif
 	case MT_NS:
 		return LOWER_ATTRS(NS);
+	/* For a Block or Page descriptor fetched using the EL2 stage 1 or EL2&0
+	 * stage 1 translation regimes in the Realm Security state, bit 5 is the NS
+	 * field. -- ARM VMSA Manual */
 	default: /* MT_SECURE */
 		return 0U;
 	}
diff --git a/lib/xlat_tables_v2/xlat_tables_context.c b/lib/xlat_tables_v2/xlat_tables_context.c
index 95dae88eb..381cbaef6 100644
--- a/lib/xlat_tables_v2/xlat_tables_context.c
+++ b/lib/xlat_tables_v2/xlat_tables_context.c
@@ -61,6 +61,88 @@ void mmap_add_alloc_va(mmap_region_t *mm)
 }
 
 #if PLAT_XLAT_TABLES_DYNAMIC
+void rmm_remap_xlat_table_entry(unsigned long long va_addr, unsigned int attr)
+{
+	VERBOSE("rmm_remap_xlat_table_entry: 0x%llx\n", va_addr);
+
+	xlat_ctx_t *ctx = &tf_xlat_ctx;
+	uint64_t desc;
+	int i = 0;
+	unsigned int l1_table_index = 0, l2_table_index = 0, l3_table_index = 0;
+	uint64_t *base_table;
+
+	for (i = 0; i < 0x4; i++){
+		VERBOSE("L1 page tables desc: 0x%lx\n", ctx->base_table[i]);
+	}
+
+	VERBOSE("ALL availalable tables\n");
+	for (i = 0; i < ctx->tables_num; ++i) {
+		VERBOSE("page tables: %p in use [%d]\n", ctx->tables[i], ctx->tables_mapped_regions[i]);
+	}
+
+	l1_table_index = XLAT_TABLE_IDX(va_addr, 1);
+	desc = ctx->base_table[l1_table_index];
+	if((desc & DESC_MASK) != INVALID_DESC){
+		remap_l2_block_entry(ctx, desc & TABLE_ADDR_MASK, va_addr, attr);
+	}
+	else{
+		ERROR("Wrong L1 table index for 0x%llx\n", va_addr);
+		assert(0);
+	}
+
+	VERBOSE("L1 availalable tables after remapiing\n");
+	for (i = 0; i < ctx->tables_num; ++i) {
+		VERBOSE("L1 page tables: %p in use [%d]\n", ctx->tables[i], ctx->tables_mapped_regions[i]);
+	}
+
+	l1_table_index = XLAT_TABLE_IDX(va_addr, 1);
+	desc = ctx->base_table[l1_table_index];
+	VERBOSE("L1 page table: %p[%d] with 0x%lx\n", ctx->base_table, l1_table_index, desc);
+
+	base_table = (uint64_t*) ( desc & TABLE_ADDR_MASK);
+	l2_table_index = XLAT_TABLE_IDX(va_addr, 2);
+	desc = *(base_table + l2_table_index);
+	VERBOSE("L2 page table: %p[%d] with 0x%lx\n", base_table, l2_table_index, desc);
+
+	base_table = (uint64_t*)(desc & TABLE_ADDR_MASK);
+	l3_table_index = XLAT_TABLE_IDX(va_addr, 3);
+	desc = *(base_table + l3_table_index);
+	VERBOSE("L3 page table: %p[%d] with 0x%lx\n\n", base_table, l3_table_index, desc);
+}
+
+void rmm_undo_remap_xlat_table_entry(unsigned long long va_addr)
+{
+	VERBOSE("rmm_undo_remap_xlat_table_entry: 0x%llx\n", va_addr);
+	xlat_ctx_t *ctx = &tf_xlat_ctx;
+	unsigned int l1_table_index = 0, l2_table_index = 0;
+	uint64_t *base_table;
+	uint64_t desc;
+	int i = 0;
+	VERBOSE("L1 availalable tables\n");
+	for (i = 0; i < ctx->tables_num; ++i) {
+		VERBOSE("page tables: %p in use [%d]\n", ctx->tables[i], ctx->tables_mapped_regions[i]);
+	}
+
+	l1_table_index = XLAT_TABLE_IDX(va_addr, 1);
+	desc = ctx->base_table[l1_table_index];
+
+	base_table = (uint64_t*) ( desc & TABLE_ADDR_MASK);
+	l2_table_index = XLAT_TABLE_IDX(va_addr, 2);
+	desc = *(base_table + l2_table_index);
+
+	if((desc & DESC_MASK) & BLOCK_DESC){
+		undo_remap_l2_block_entry(ctx, (uint64_t)base_table, va_addr);
+	}
+	else{
+		ERROR("Wrong L2 table index for 0x%llx\n", va_addr);
+		assert(0);
+	}
+
+	VERBOSE("L1 availalable tables after undo remaping\n");
+	for (i = 0; i < ctx->tables_num; ++i) {
+		VERBOSE("L1 page tables: %p in use [%d]\n", ctx->tables[i], ctx->tables_mapped_regions[i]);
+	}
+}
 
 int mmap_add_dynamic_region(unsigned long long base_pa, uintptr_t base_va,
 			    size_t size, unsigned int attr)
diff --git a/lib/xlat_tables_v2/xlat_tables_core.c b/lib/xlat_tables_v2/xlat_tables_core.c
index de5718454..ea28dcdd0 100644
--- a/lib/xlat_tables_v2/xlat_tables_core.c
+++ b/lib/xlat_tables_v2/xlat_tables_core.c
@@ -755,11 +755,7 @@ static int mmap_add_region_check(const xlat_ctx_t *ctx, const mmap_region_t *mm)
 			 * Partial overlaps are not allowed
 			 */
 
-			unsigned long long mm_cursor_end_pa =
-				     mm_cursor->base_pa + mm_cursor->size - 1U;
-
-			bool separated_pa = (end_pa < mm_cursor->base_pa) ||
-				(base_pa > mm_cursor_end_pa);
+			bool separated_pa = true;
 			bool separated_va = (end_va < mm_cursor->base_va) ||
 				(base_va > mm_cursor_end_va);
 
@@ -1183,6 +1179,80 @@ void xlat_setup_dynamic_ctx(xlat_ctx_t *ctx, unsigned long long pa_max,
 	ctx->initialized = 0;
 }
 
+void remap_l2_block_entry(xlat_ctx_t *ctx, uintptr_t table_base_va, uint64_t va_addr, unsigned int attr)
+{
+	VERBOSE("remap_l2_block_entry: L2 table base [0x%lx] for virtual address [0x%lx]\n", table_base_va, va_addr);
+	uint64_t new_desc, old_desc, tmp_addr;
+	uint64_t *l2_table_ptr = (uint64_t*)(table_base_va);
+	uint64_t *sub_table;
+	uint64_t start_va_addr, end_va_addr;
+	int l2_table_index = XLAT_TABLE_IDX(va_addr, 2), l3_table_index;
+	unsigned int old_attr;
+
+	sub_table = xlat_table_get_empty(ctx);
+	if(sub_table == NULL){
+		ERROR("Null sub_table\n");
+		assert(0);
+	}
+
+	old_desc = l2_table_ptr[l2_table_index];
+	old_attr = MT_MEMORY | MT_RW | ATTR_INDEX_GET(old_desc);
+
+	//Fill the new map with original attribute including the target entry
+	start_va_addr = va_addr & XLAT_ADDR_MASK(2);
+	end_va_addr = start_va_addr + XLAT_BLOCK_SIZE(2);
+	for(uint64_t cur_addr = start_va_addr; cur_addr < end_va_addr; cur_addr += XLAT_BLOCK_SIZE(3)){
+		tmp_addr = cur_addr & XLAT_ADDR_MASK(3);
+		new_desc = xlat_desc(ctx, old_attr, tmp_addr, 3);
+		l3_table_index = XLAT_TABLE_IDX(cur_addr, 3);
+		sub_table[l3_table_index] = new_desc;
+		xlat_arch_tlbi_va(tmp_addr, ctx->xlat_regime);
+	}
+
+	// reset the target entry with target attribute
+	tmp_addr = va_addr & XLAT_ADDR_MASK(3);
+	new_desc = xlat_desc(ctx, attr, tmp_addr, 3);
+	l3_table_index = XLAT_TABLE_IDX(va_addr, 3);
+	sub_table[l3_table_index] = new_desc;
+	xlat_arch_tlbi_va(tmp_addr, ctx->xlat_regime);
+	xlat_table_inc_regions_count(ctx, sub_table);
+
+	new_desc = (uintptr_t)(sub_table) | TABLE_DESC;
+	l2_table_ptr[l2_table_index] = new_desc;
+}
+
+void undo_remap_l2_block_entry(xlat_ctx_t *ctx, uintptr_t table_base_va, uint64_t va_addr)
+{
+	VERBOSE("undo_remap_l2_block_entry: L2 table base [0x%lx] for virtual address [0x%lx]\n", table_base_va, va_addr);
+	uint64_t new_desc;
+	int l2_table_index = XLAT_TABLE_IDX(va_addr, 2), l3_table_index;
+	uint64_t *l2_table_ptr = (uint64_t*)(table_base_va);
+	uint64_t desc = l2_table_ptr[l2_table_index];
+	uint64_t *sub_table = (uint64_t*)(desc & TABLE_ADDR_MASK);
+	uint64_t start_va_addr, end_va_addr;
+	if((desc & DESC_MASK) != TABLE_DESC){
+		ERROR("Expecting a table descriptor at %p[%d]: 0x%lx\n", l2_table_ptr, l2_table_index, desc);
+		assert(0);
+	}
+	//set the new entry description
+	new_desc = va_addr & XLAT_ADDR_MASK(2);
+	new_desc |= BLOCK_DESC;
+	new_desc |= (MT_MEMORY | MT_REALM | MT_RW) << 5;
+	new_desc |= (UL(1) << XN_SHIFT);
+	VERBOSE("new desc: 0x%lx\n", new_desc);
+	l2_table_ptr[l2_table_index] = new_desc;
+	//Decrease the ref count of the released table and flush all TLB
+	VERBOSE("L2 table address: 0x%llx for 0x%lx\n", desc & TABLE_ADDR_MASK, va_addr);
+	start_va_addr = va_addr & XLAT_ADDR_MASK(2);
+	end_va_addr = start_va_addr + XLAT_BLOCK_SIZE(2);
+	for(uint64_t cur_addr = start_va_addr; cur_addr < end_va_addr; cur_addr += XLAT_BLOCK_SIZE(3)){
+		l3_table_index = XLAT_TABLE_IDX(cur_addr, 3);
+		sub_table[l3_table_index] = INVALID_DESC;
+		// xlat_arch_tlbi_va(cur_addr, ctx->xlat_regime);
+	}
+	xlat_table_dec_regions_count(ctx, sub_table);
+}
+
 #endif /* PLAT_XLAT_TABLES_DYNAMIC */
 
 void __init init_xlat_tables_ctx(xlat_ctx_t *ctx)
diff --git a/make_helpers/defaults.mk b/make_helpers/defaults.mk
index 6e572377b..f5b99df5c 100644
--- a/make_helpers/defaults.mk
+++ b/make_helpers/defaults.mk
@@ -114,6 +114,9 @@ ENABLE_PSCI_STAT		:= 0
 # Flag to enable Realm Management Extension (FEAT_RME)
 ENABLE_RME			:= 0
 
+# Flag to relax checks in RMM.
+RME_DEBUG			:= 0
+
 # Flag to enable runtime instrumentation using PMF
 ENABLE_RUNTIME_INSTRUMENTATION	:= 0
 
@@ -460,3 +463,7 @@ ENABLE_TRF_FOR_NS		:= 0
 # SCR_EL3.TWEDEL(4bit) field, when FEAT_TWED is implemented.
 # By default it takes 0, and need to be updated by the platforms.
 TWED_DELAY			:= 0
+
+# For now, the feature of SMMU is not fully implemented in QEMU platform
+# Set this feature when implementing related features
+ENABLE_SMMU			:= 0
diff --git a/plat/arm/board/fvp/include/platform_def.h b/plat/arm/board/fvp/include/platform_def.h
index 82bd7c8a5..002bf9162 100644
--- a/plat/arm/board/fvp/include/platform_def.h
+++ b/plat/arm/board/fvp/include/platform_def.h
@@ -46,6 +46,14 @@
 #if ENABLE_RME
 #define PLAT_ARM_RMM_BASE		(RMM_BASE)
 #define PLAT_ARM_RMM_SIZE		(RMM_LIMIT - RMM_BASE)
+
+#define PLAT_ARM_RMM_CODE_BASE	(BL_CODE_BASE)
+#define PLAT_ARM_RMM_CODE_LIMIT	(BL_CODE_END)
+#define PLAT_ARM_RMM_CODE_SIZE  (PLAT_ARM_RMM_CODE_LIMIT - PLAT_ARM_RMM_CODE_BASE)
+
+#define PLAT_ARM_RMM_RODATA_BASE	(PLAT_ARM_RMM_CODE_LIMIT)
+#define PLAT_ARM_RMM_RODATA_SIZE	UL(0x1000)
+#define PLAT_ARM_RMM_RODATA_LIMIT	(PLAT_ARM_RMM_RODATA_BASE + PLAT_ARM_RMM_RODATA_SIZE)
 #endif
 
 /*
diff --git a/plat/arm/common/trp/arm_trp_setup.c b/plat/arm/common/trp/arm_trp_setup.c
index 8e4829344..6b475f03d 100644
--- a/plat/arm/common/trp/arm_trp_setup.c
+++ b/plat/arm/common/trp/arm_trp_setup.c
@@ -34,7 +34,47 @@ void arm_trp_early_platform_setup(void)
 			  CONSOLE_FLAG_BOOT | CONSOLE_FLAG_RUNTIME);
 }
 
+void arm_trp_plat_arch_setup(void)
+{
+	INFO("TRP: set up plat arch\n");
+
+	const mmap_region_t bl_regions[] = {
+		ARM_MAP_RMM_DRAM,
+		ARM_MAP_RMM_CODE,
+		ARM_MAP_RMM_RODATA,
+		{0}
+	};
+
+	setup_page_tables(bl_regions, plat_arm_get_mmap());
+
+	/* TRP runs in EL2 when RME enabled. */
+	enable_mmu_el2(0);
+
+	int ret = mmap_add_dynamic_region(0x80000000, 0x80000000, 0x3f000000,
+			MT_MEMORY|MT_RW|MT_REALM);
+	if (ret) {
+		ERROR("arm_trp_plat_arch_setup: failed to set up dynamic region: %d\n", ret);
+	}
+	ret = mmap_add_dynamic_region(0xbf000000, 0xbf000000, 0x1000000,
+			MT_MEMORY|MT_RW|MT_REALM);
+	if (ret) {
+		ERROR("arm_trp_plat_arch_setup: failed to set up dynamic region: %d\n", ret);
+	}
+}
+
 void trp_early_platform_setup(void)
 {
 	arm_trp_early_platform_setup();
 }
+
+void trp_plat_arch_setup(void)
+{
+	arm_trp_plat_arch_setup();
+}
+
+void trp_plat_arch_enable_mmu(int id)
+{
+	if (!(read_sctlr_el2() & SCTLR_M_BIT)) {
+		enable_mmu_el2(0);
+	}
+}
diff --git a/plat/qemu/common/qemu_bl1_setup.c b/plat/qemu/common/qemu_bl1_setup.c
index 67f33273f..4ad0450c6 100644
--- a/plat/qemu/common/qemu_bl1_setup.c
+++ b/plat/qemu/common/qemu_bl1_setup.c
@@ -11,6 +11,7 @@
 #include <arch.h>
 #include <arch_helpers.h>
 #include <common/bl_common.h>
+#include <drivers/arm/smmu_v3.h>
 
 #include "qemu_private.h"
 
@@ -59,4 +60,8 @@ void bl1_plat_arch_setup(void)
 void bl1_platform_setup(void)
 {
 	plat_qemu_io_setup();
+#if ENABLE_SMMU
+	INFO("BL1: SMMUV3 init SMMU_S\n");
+	smmuv3_security_init(PLAT_QEMU_SMMUV3_BASE);
+#endif
 }
diff --git a/plat/qemu/common/qemu_bl2_mem_params_desc.c b/plat/qemu/common/qemu_bl2_mem_params_desc.c
index 5af3a2264..50723118d 100644
--- a/plat/qemu/common/qemu_bl2_mem_params_desc.c
+++ b/plat/qemu/common/qemu_bl2_mem_params_desc.c
@@ -55,11 +55,29 @@ static bl_mem_params_node_t bl2_mem_params_descs[] = {
 
 # ifdef QEMU_LOAD_BL32
 	  .next_handoff_image_id = BL32_IMAGE_ID,
+# elif ENABLE_RME
+	  .next_handoff_image_id = RMM_IMAGE_ID,
 # else
 	  .next_handoff_image_id = BL33_IMAGE_ID,
 # endif
 	},
 #endif /* __aarch64__ */
+
+# if ENABLE_RME
+	/* Fill RMM related information */
+	{
+		.image_id = RMM_IMAGE_ID,
+		SET_STATIC_PARAM_HEAD(ep_info, PARAM_EP,
+				VERSION_2, entry_point_info_t, EP_REALM | EXECUTABLE),
+		.ep_info.pc = RMM_BASE,
+		SET_STATIC_PARAM_HEAD(image_info, PARAM_EP,
+				VERSION_2, image_info_t, 0),
+		.image_info.image_base = RMM_BASE,
+		.image_info.image_max_size = RMM_LIMIT - RMM_BASE,
+		.next_handoff_image_id = BL33_IMAGE_ID,
+	},
+# endif
+
 # ifdef QEMU_LOAD_BL32
 
 #ifdef __aarch64__
@@ -83,7 +101,11 @@ static bl_mem_params_node_t bl2_mem_params_descs[] = {
 	  .image_info.image_base = BL32_BASE,
 	  .image_info.image_max_size = BL32_LIMIT - BL32_BASE,
 
-	  .next_handoff_image_id = BL33_IMAGE_ID,
+# if ENABLE_RME
+	  .next_handoff_image_id = RMM_IMAGE_ID,
+# else
+	  .next_handoff_image_id = BL33_IMAAE_ID,
+# endif
 	},
 
 	/*
diff --git a/plat/qemu/common/qemu_bl2_setup.c b/plat/qemu/common/qemu_bl2_setup.c
index 2c0da15b9..2528f5e05 100644
--- a/plat/qemu/common/qemu_bl2_setup.c
+++ b/plat/qemu/common/qemu_bl2_setup.c
@@ -19,6 +19,7 @@
 #include <lib/optee_utils.h>
 #include <lib/utils.h>
 #include <plat/common/platform.h>
+#include <lib/gpt_rme/gpt_rme.h>
 
 #include "qemu_private.h"
 
@@ -82,8 +83,48 @@ void bl2_platform_setup(void)
 	/* TODO Initialize timer */
 }
 
+#if ENABLE_RME
+static void qemu_bl2_plat_gpt_setup(void)
+{
+    pas_region_t pas_regions[] = {
+        QEMU_PAS1_GPI_ANY,
+        QEMU_PAS2_GPI_ANY,
+        QEMU_PAS_KERNEL,
+        QEMU_PAS_REALM
+    };
+
+	INFO("qemu bl2 plat gpt setup called!\n");
+    if (gpt_init_l0_tables(GPCCR_PPS_4GB, QEMU_L0_GPT_ADDR_BASE,
+        QEMU_L0_GPT_SIZE) < 0) {
+        ERROR("gpt_init_l0_tables() failed!\n");
+        panic();
+    }
+
+    if (gpt_init_pas_l1_tables(GPCCR_PGS_4K,
+                                QEMU_L1_GPT_ADDR_BASE,
+                                QEMU_L1_GPT_SIZE,
+                                pas_regions,
+                                (unsigned int)(sizeof(pas_regions) /
+                                sizeof(pas_region_t))) < 0) {
+        ERROR("gpt_init_pas_l1_tables() failed!\n");
+        panic();
+    }
+
+    INFO("Enabling Granule Protection Checks\n");
+    if (gpt_enable() < 0) {
+        ERROR("gpt_enable() failed!\n");
+        panic();
+    }
+	INFO("Finish Enabling GPT\n");
+}
+#endif
+
 #ifdef __aarch64__
+#if ENABLE_RME
+#define QEMU_CONFIGURE_BL2_MMU(...)	qemu_configure_mmu_el3(__VA_ARGS__)
+#else
 #define QEMU_CONFIGURE_BL2_MMU(...)	qemu_configure_mmu_el1(__VA_ARGS__)
+#endif
 #else
 #define QEMU_CONFIGURE_BL2_MMU(...)	qemu_configure_mmu_svc_mon(__VA_ARGS__)
 #endif
@@ -95,6 +136,10 @@ void bl2_plat_arch_setup(void)
 			      BL_CODE_BASE, BL_CODE_END,
 			      BL_RO_DATA_BASE, BL_RO_DATA_END,
 			      BL_COHERENT_RAM_BASE, BL_COHERENT_RAM_END);
+
+#if ENABLE_RME
+	qemu_bl2_plat_gpt_setup();
+#endif
 }
 
 /*******************************************************************************
diff --git a/plat/qemu/common/qemu_bl31_setup.c b/plat/qemu/common/qemu_bl31_setup.c
index 4f60eb163..4216fdf0a 100644
--- a/plat/qemu/common/qemu_bl31_setup.c
+++ b/plat/qemu/common/qemu_bl31_setup.c
@@ -8,7 +8,12 @@
 
 #include <common/bl_common.h>
 #include <drivers/arm/pl061_gpio.h>
+#include <drivers/arm/smmu_v3.h>
+#include <lib/mmio.h>
 #include <plat/common/platform.h>
+#if ENABLE_RME
+#include <lib/gpt_rme/gpt_rme.h>
+#endif
 
 #include "qemu_private.h"
 
@@ -18,6 +23,9 @@
  */
 static entry_point_info_t bl32_image_ep_info;
 static entry_point_info_t bl33_image_ep_info;
+#if ENABLE_RME
+static entry_point_info_t rmm_image_ep_info;
+#endif
 
 /*******************************************************************************
  * Perform any BL3-1 early platform setup.  Here is an opportunity to copy
@@ -45,13 +53,18 @@ void bl31_early_platform_setup2(u_register_t arg0, u_register_t arg1,
 	bl_params_node_t *bl_params = params_from_bl2->head;
 
 	/*
-	 * Copy BL33 and BL32 (if present), entry point information.
+	 * Copy BL33, BL32 and RMM (if present), entry point information.
 	 * They are stored in Secure RAM, in BL2's address space.
 	 */
 	while (bl_params) {
 		if (bl_params->image_id == BL32_IMAGE_ID)
 			bl32_image_ep_info = *bl_params->ep_info;
 
+#if ENABLE_RME
+		if (bl_params->image_id == RMM_IMAGE_ID)
+			rmm_image_ep_info = *bl_params->ep_info;
+#endif
+
 		if (bl_params->image_id == BL33_IMAGE_ID)
 			bl33_image_ep_info = *bl_params->ep_info;
 
@@ -60,6 +73,14 @@ void bl31_early_platform_setup2(u_register_t arg0, u_register_t arg1,
 
 	if (!bl33_image_ep_info.pc)
 		panic();
+#if ENABLE_RME
+	if (rmm_image_ep_info.pc == 0U)
+		panic();
+#endif
+#if ENABLE_SMMU
+	INFO("BL31: SMMUV3 init SMMU_ROOT\n");
+	smmuv3_init(PLAT_QEMU_SMMUV3_BASE);
+#endif
 }
 
 void bl31_plat_arch_setup(void)
@@ -68,6 +89,19 @@ void bl31_plat_arch_setup(void)
 			      BL_CODE_BASE, BL_CODE_END,
 			      BL_RO_DATA_BASE, BL_RO_DATA_END,
 			      BL_COHERENT_RAM_BASE, BL_COHERENT_RAM_END);
+
+#if ENABLE_RME
+	/*
+	 * Initialise Granule Protection library and enable GPC for the primary
+	 * processor. The tables have already been initialized by a previous BL
+	 * stage, so there is no need to provide any PAS here. This function
+	 * sets up pointers to those tables.
+	 */
+	if (gpt_runtime_init() < 0) {
+		ERROR("gpt_runtime_init() failed!\n");
+		panic();
+	}
+#endif /* ENABLE_RME */
 }
 
 static void qemu_gpio_init(void)
@@ -100,8 +134,22 @@ entry_point_info_t *bl31_plat_get_next_image_ep_info(uint32_t type)
 	entry_point_info_t *next_image_info;
 
 	assert(sec_state_is_valid(type));
+	/*
 	next_image_info = (type == NON_SECURE)
 			? &bl33_image_ep_info : &bl32_image_ep_info;
+	*/
+	if (type == NON_SECURE) {
+		next_image_info = &bl33_image_ep_info;
+	}
+#if ENABLE_RME
+	else if (type == REALM) {
+		next_image_info = &rmm_image_ep_info;
+	}
+#endif
+	else {
+		next_image_info = &bl32_image_ep_info;
+	}
+
 	/*
 	 * None of the images on the ARM development platforms can have 0x0
 	 * as the entrypoint
diff --git a/plat/qemu/common/qemu_common.c b/plat/qemu/common/qemu_common.c
index 0c184f49b..7664560c5 100644
--- a/plat/qemu/common/qemu_common.c
+++ b/plat/qemu/common/qemu_common.c
@@ -14,6 +14,19 @@
 #include <plat/common/platform.h>
 #include "qemu_private.h"
 
+#define MAP_GPT_L0	MAP_REGION_FLAT(QEMU_L0_GPT_ADDR_BASE,			\
+					QEMU_L0_GPT_SIZE,			\
+					MT_MEMORY | MT_RW | MT_ROOT)
+
+#define MAP_GPT_L1	MAP_REGION_FLAT(QEMU_L1_GPT_ADDR_BASE,			\
+					QEMU_L1_GPT_SIZE,			\
+					MT_MEMORY | MT_RW | MT_ROOT)
+
+#if ENABLE_RME
+#define MAP_REALM   MAP_REGION_FLAT(RMM_BASE, RMM_SIZE, \
+                    MT_MEMORY | MT_RW | MT_REALM)
+#endif
+
 #define MAP_DEVICE0	MAP_REGION_FLAT(DEVICE0_BASE,			\
 					DEVICE0_SIZE,			\
 					MT_DEVICE | MT_RW | MT_SECURE)
@@ -65,6 +78,7 @@ static const mmap_region_t plat_qemu_mmap[] = {
 #endif
 	{0}
 };
+#define MT_IMAGE_PAS	MT_ROOT
 #endif
 #ifdef IMAGE_BL2
 static const mmap_region_t plat_qemu_mmap[] = {
@@ -72,6 +86,11 @@ static const mmap_region_t plat_qemu_mmap[] = {
 	MAP_FLASH1,
 	MAP_SHARED_RAM,
 	MAP_DEVICE0,
+	MAP_GPT_L0,
+	MAP_GPT_L1,
+#if ENABLE_RME
+	MAP_REALM,
+#endif
 #ifdef MAP_DEVICE1
 	MAP_DEVICE1,
 #endif
@@ -86,11 +105,17 @@ static const mmap_region_t plat_qemu_mmap[] = {
 #endif
 	{0}
 };
+#define MT_IMAGE_PAS	MT_ROOT
 #endif
 #ifdef IMAGE_BL31
 static const mmap_region_t plat_qemu_mmap[] = {
 	MAP_SHARED_RAM,
 	MAP_DEVICE0,
+	MAP_GPT_L0,
+	MAP_GPT_L1,
+#if ENABLE_RME
+	MAP_REALM,
+#endif
 #ifdef MAP_DEVICE1
 	MAP_DEVICE1,
 #endif
@@ -105,6 +130,7 @@ static const mmap_region_t plat_qemu_mmap[] = {
 #endif
 	{0}
 };
+#define MT_IMAGE_PAS	MT_ROOT
 #endif
 #ifdef IMAGE_BL32
 static const mmap_region_t plat_qemu_mmap[] = {
@@ -118,6 +144,22 @@ static const mmap_region_t plat_qemu_mmap[] = {
 #endif
 	{0}
 };
+#define MT_IMAGE_PAS	MT_SECURE
+#endif
+
+#ifdef IMAGE_RMM
+static const mmap_region_t plat_qemu_mmap[] = {
+	MAP_SHARED_RAM,
+	MAP_DEVICE0,
+#ifdef MAP_DEVICE1
+	MAP_DEVICE1,
+#endif
+#ifdef MAP_DEVICE2
+	MAP_DEVICE2,
+#endif
+	{0}
+};
+#define MT_IMAGE_PAS	MT_REALM
 #endif
 
 /*******************************************************************************
@@ -137,16 +179,16 @@ static const mmap_region_t plat_qemu_mmap[] = {
 	{								\
 		mmap_add_region(total_base, total_base,			\
 				total_size,				\
-				MT_MEMORY | MT_RW | MT_SECURE);		\
+				MT_MEMORY | MT_RW | MT_IMAGE_PAS);		\
 		mmap_add_region(code_start, code_start,			\
 				code_limit - code_start,		\
-				MT_CODE | MT_SECURE);			\
+				MT_CODE | MT_IMAGE_PAS);			\
 		mmap_add_region(ro_start, ro_start,			\
 				ro_limit - ro_start,			\
-				MT_RO_DATA | MT_SECURE);		\
+				MT_RO_DATA | MT_IMAGE_PAS);		\
 		mmap_add_region(coh_start, coh_start,			\
 				coh_limit - coh_start,			\
-				MT_DEVICE | MT_RW | MT_SECURE);		\
+				MT_DEVICE | MT_RW | MT_IMAGE_PAS);		\
 		mmap_add(plat_qemu_mmap);				\
 		init_xlat_tables();					\
 									\
@@ -156,6 +198,7 @@ static const mmap_region_t plat_qemu_mmap[] = {
 /* Define EL1 and EL3 variants of the function initialising the MMU */
 #ifdef __aarch64__
 DEFINE_CONFIGURE_MMU_EL(el1)
+DEFINE_CONFIGURE_MMU_EL(el2)
 DEFINE_CONFIGURE_MMU_EL(el3)
 #else
 DEFINE_CONFIGURE_MMU_EL(svc_mon)
diff --git a/plat/qemu/common/qemu_io_storage.c b/plat/qemu/common/qemu_io_storage.c
index 1107e443f..686a318de 100644
--- a/plat/qemu/common/qemu_io_storage.c
+++ b/plat/qemu/common/qemu_io_storage.c
@@ -27,6 +27,9 @@
 #define BL32_EXTRA1_IMAGE_NAME		"bl32_extra1.bin"
 #define BL32_EXTRA2_IMAGE_NAME		"bl32_extra2.bin"
 #define BL33_IMAGE_NAME			"bl33.bin"
+#if ENABLE_RME
+#define RMM_IMAGE_NAME			"rmm.bin"
+#endif
 
 #if TRUSTED_BOARD_BOOT
 #define TRUSTED_BOOT_FW_CERT_NAME	"tb_fw.crt"
@@ -82,6 +85,12 @@ static const io_uuid_spec_t bl33_uuid_spec = {
 	.uuid = UUID_NON_TRUSTED_FIRMWARE_BL33,
 };
 
+#if ENABLE_RME
+static const io_uuid_spec_t rmm_uuid_spec = {
+	.uuid = UUID_REALM_MONITOR_MGMT_FIRMWARE,
+};
+#endif
+
 #if TRUSTED_BOARD_BOOT
 static const io_uuid_spec_t tb_fw_cert_uuid_spec = {
 	.uuid = UUID_TRUSTED_BOOT_FW_CERT,
@@ -141,6 +150,12 @@ static const io_file_spec_t sh_file_spec[] = {
 		.path = BL33_IMAGE_NAME,
 		.mode = FOPEN_MODE_RB
 	},
+#if ENABLE_RME
+	[RMM_IMAGE_ID] = {
+		.path = RMM_IMAGE_NAME,
+		.mode = FOPEN_MODE_RB
+	},
+#endif
 #if TRUSTED_BOARD_BOOT
 	[TRUSTED_BOOT_FW_CERT_ID] = {
 		.path = TRUSTED_BOOT_FW_CERT_NAME,
@@ -251,6 +266,13 @@ static const struct plat_io_policy policies[] = {
 		(uintptr_t)&bl32_extra2_uuid_spec,
 		open_fip
 	},
+#endif
+#if ENABLE_RME
+	[RMM_IMAGE_ID] = {
+		&fip_dev_handle,
+		(uintptr_t)&rmm_uuid_spec,
+		open_fip
+	},
 #endif
 	[BL33_IMAGE_ID] = {
 		&fip_dev_handle,
diff --git a/plat/qemu/common/qemu_plat_attest_token.c b/plat/qemu/common/qemu_plat_attest_token.c
new file mode 100644
index 000000000..604fbc764
--- /dev/null
+++ b/plat/qemu/common/qemu_plat_attest_token.c
@@ -0,0 +1,22 @@
+/*
+ * Copyright (c) 2022, ARM Limited and Contributors. All rights reserved.
+ *
+ * SPDX-License-Identifier: BSD-3-Clause
+ */
+
+#include <errno.h>
+#include <stdint.h>
+#include <string.h>
+
+int plat_get_cca_attest_token(uintptr_t buf, size_t *len,
+                               uintptr_t hash, size_t hash_size)
+{
+	if (*len < 8) {
+		return -EINVAL;
+	}
+	//[TODO] Temporarily add invalid value, to implement in future
+	memset((void *)buf, 0, 8);
+	*len = 8;
+
+	return 0;
+}
diff --git a/plat/qemu/common/qemu_private.h b/plat/qemu/common/qemu_private.h
index c313cb63f..70815c064 100644
--- a/plat/qemu/common/qemu_private.h
+++ b/plat/qemu/common/qemu_private.h
@@ -20,6 +20,11 @@ void qemu_configure_mmu_el1(unsigned long total_base, unsigned long total_size,
 			unsigned long ro_start, unsigned long ro_limit,
 			unsigned long coh_start, unsigned long coh_limit);
 
+void qemu_configure_mmu_el2(unsigned long total_base, unsigned long total_size,
+			unsigned long code_start, unsigned long code_limit,
+			unsigned long ro_start, unsigned long ro_limit,
+			unsigned long coh_start, unsigned long coh_limit);
+
 void qemu_configure_mmu_el3(unsigned long total_base, unsigned long total_size,
 			unsigned long code_start, unsigned long code_limit,
 			unsigned long ro_start, unsigned long ro_limit,
diff --git a/plat/qemu/common/qemu_realm_attest_key.c b/plat/qemu/common/qemu_realm_attest_key.c
new file mode 100644
index 000000000..0f6465cff
--- /dev/null
+++ b/plat/qemu/common/qemu_realm_attest_key.c
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2022, ARM Limited and Contributors. All rights reserved.
+ *
+ * SPDX-License-Identifier: BSD-3-Clause
+ */
+
+#include <assert.h>
+#include <errno.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+#include <services/rmmd_svc.h>
+
+int plat_get_cca_realm_attest_key(uintptr_t buf, size_t *len, unsigned int type)
+{
+        assert(type == ATTEST_KEY_CURVE_ECC_SECP384R1);
+
+        if (*len < 0) {
+                return -EINVAL;
+        }
+
+        //[TODO] Temporarily add invalid value, to implement in future
+        (void)memcpy((void *)buf, "0000", 4);
+        *len = 4;
+
+        return 0;
+}
+
diff --git a/plat/qemu/common/trp/qemu_trp.mk b/plat/qemu/common/trp/qemu_trp.mk
new file mode 100644
index 000000000..35e2c50da
--- /dev/null
+++ b/plat/qemu/common/trp/qemu_trp.mk
@@ -0,0 +1,3 @@
+# TRP SOURCE files common to QEMU standard platforms
+RMM_SOURCES	+= plat/qemu/common/trp/qemu_trp_setup.c	\
+		   plat/common/aarch64/platform_mp_stack.S
diff --git a/plat/qemu/common/trp/qemu_trp_setup.c b/plat/qemu/common/trp/qemu_trp_setup.c
new file mode 100644
index 000000000..5d395c9f3
--- /dev/null
+++ b/plat/qemu/common/trp/qemu_trp_setup.c
@@ -0,0 +1,55 @@
+#include <assert.h>
+#include <platform_def.h>
+#include <drivers/console.h>
+#include <drivers/arm/pl011.h>
+#include <plat/common/platform.h>
+#include <lib/xlat_tables/xlat_tables_v2.h>
+
+#include "../qemu_private.h"
+
+static console_t qemu_trp_runtime_console;
+
+void qemu_trp_early_platform_setup(void)
+{
+	/*TODO trp should use another UART other than the BOOT UART */
+	(void)console_pl011_register(PLAT_QEMU_BOOT_UART_BASE,
+			PLAT_QEMU_BOOT_UART_CLK_IN_HZ,
+			PLAT_QEMU_CONSOLE_BAUDRATE, &qemu_trp_runtime_console);
+
+	console_set_scope(&qemu_trp_runtime_console, CONSOLE_FLAG_BOOT |
+			CONSOLE_FLAG_RUNTIME);
+}
+
+void qemu_trp_plat_arch_setup(void)
+{
+	qemu_configure_mmu_el2(RMM_BASE, RMM_SIZE,
+					RMM_CODE_BASE, RMM_CODE_LIMIT,
+					RMM_RO_DATA_BASE, RMM_RO_DATA_LIMIT,
+					RMM_COHERENT_BASE, RMM_COHERENT_LIMIT);
+	int ret = mmap_add_dynamic_region(0x40000000, 0x40000000, 0xc0000000, MT_MEMORY|MT_RW|MT_REALM);
+	if (ret) {
+		ERROR("arm_trp_plat_arch_setup: setting up dynamic region 0x40000000: %d\n", ret);
+	}
+	ret = mmap_add_dynamic_region(0x40000000, 0x140000000, 0xc0000000, MT_MEMORY|MT_RW|MT_NS);
+	if (ret) {
+		ERROR("arm_trp_plat_arch_setup: setting up shadow dynamic region 0x40000000: %d\n", ret);
+		panic();
+	}
+}
+
+void trp_early_platform_setup(void)
+{
+	qemu_trp_early_platform_setup();
+}
+
+void trp_plat_arch_setup(void)
+{
+	qemu_trp_plat_arch_setup();
+}
+
+void trp_plat_arch_enable_mmu(int id)
+{
+	if (!(read_sctlr_el2() & SCTLR_M_BIT)) {
+		enable_mmu_el2(0);
+	}
+}
diff --git a/plat/qemu/qemu/include/platform_def.h b/plat/qemu/qemu/include/platform_def.h
index 78467c4af..3d470226c 100644
--- a/plat/qemu/qemu/include/platform_def.h
+++ b/plat/qemu/qemu/include/platform_def.h
@@ -23,18 +23,18 @@
 #define PLATFORM_CLUSTER0_CORE_COUNT	PLATFORM_MAX_CPUS_PER_CLUSTER
 #define PLATFORM_CLUSTER1_CORE_COUNT	U(0)
 #else
-#define PLATFORM_MAX_CPUS_PER_CLUSTER	U(4)
+#define PLATFORM_MAX_CPUS_PER_CLUSTER	U(8)
 /*
  * Define the number of cores per cluster used in calculating core position.
  * The cluster number is shifted by this value and added to the core ID,
  * so its value represents log2(cores/cluster).
  * Default is 2**(2) = 4 cores per cluster.
  */
-#define PLATFORM_CPU_PER_CLUSTER_SHIFT	U(2)
+#define PLATFORM_CPU_PER_CLUSTER_SHIFT	U(3)
 
-#define PLATFORM_CLUSTER_COUNT		U(2)
+#define PLATFORM_CLUSTER_COUNT		U(1)
 #define PLATFORM_CLUSTER0_CORE_COUNT	PLATFORM_MAX_CPUS_PER_CLUSTER
-#define PLATFORM_CLUSTER1_CORE_COUNT	PLATFORM_MAX_CPUS_PER_CLUSTER
+#define PLATFORM_CLUSTER1_CORE_COUNT	0 //PLATFORM_MAX_CPUS_PER_CLUSTER
 #endif
 #define PLATFORM_CORE_COUNT		(PLATFORM_CLUSTER0_CORE_COUNT + \
 					 PLATFORM_CLUSTER1_CORE_COUNT)
@@ -82,12 +82,19 @@
 
 #define NS_DRAM0_BASE			ULL(0x40000000)
 #define NS_DRAM0_SIZE			ULL(0xc0000000)
+#define NS_DRAM0_LIMIT          (NS_DRAM0_BASE + NS_DRAM0_SIZE)
 
 #define SEC_SRAM_BASE			0x0e000000
 #define SEC_SRAM_SIZE			0x00060000
 
-#define SEC_DRAM_BASE			0x0e100000
-#define SEC_DRAM_SIZE			0x00f00000
+#define QEMU_L0_GPT_ADDR_BASE       (0x0e060000)
+#define QEMU_L0_GPT_SIZE            (0x1000)
+
+/* #define SEC_DRAM_BASE			0x0e100000 */
+#define QEMU_L1_GPT_ADDR_BASE       (0x0e100000)
+#define QEMU_L1_GPT_SIZE            UL(0x00100000)  /* 1MB */
+#define SEC_DRAM_BASE			0x0e200000
+#define SEC_DRAM_SIZE			0x00e00000
 
 #define SECURE_GPIO_BASE		0x090b0000
 #define SECURE_GPIO_SIZE		0x00001000
@@ -146,10 +153,50 @@
  * Put BL3-1 at the top of the Trusted SRAM. BL31_BASE is calculated using the
  * current BL3-1 debug size plus a little space for growth.
  */
-#define BL31_BASE			(BL31_LIMIT - 0x20000)
+#define BL31_BASE			(BL31_LIMIT - 0x2f000)
 #define BL31_LIMIT			(BL_RAM_BASE + BL_RAM_SIZE)
 #define BL31_PROGBITS_LIMIT		BL1_RW_BASE
 
+#if ENABLE_RME
+#define RMM_BASE		(0x0f000000)
+#define RMM_SIZE        UL(0x100000)
+#define RMM_LIMIT		(RMM_BASE + RMM_SIZE)
+
+#define RMM_CODE_BASE (BL_CODE_BASE)
+#define RMM_CODE_LIMIT (BL_CODE_END)
+#define RMM_CODE_SIZE (RMM_CODE_LIMIT - RMM_CODE_BASE)
+
+#define RMM_RO_DATA_BASE RMM_CODE_LIMIT
+#define RMM_RO_DATA_SIZE (0x1000)
+#define RMM_RO_DATA_LIMIT (RMM_RO_DATA_BASE + RMM_RO_DATA_SIZE)
+
+#define RMM_COHERENT_BASE UL(0x40120000)
+#define RMM_COHERENT_LIMIT UL(0x40120000)
+
+#define QEMU_PAS_1_BASE     (U(0))
+#define QEMU_PAS_1_SIZE     (RMM_BASE)
+#define QEMU_PAS_2_BASE     (RMM_LIMIT)
+#define QEMU_PAS_2_SIZE     (0x40000000 - QEMU_PAS_2_BASE)
+#define QEMU_PAS_REALM_BASE RMM_BASE
+#define QEMU_PAS_REALM_SIZE RMM_SIZE
+
+#define QEMU_PAS1_GPI_ANY    GPT_MAP_REGION_GRANULE(QEMU_PAS_1_BASE, \
+                             QEMU_PAS_1_SIZE, \
+                             GPT_GPI_ANY)
+
+#define QEMU_PAS2_GPI_ANY    GPT_MAP_REGION_GRANULE(QEMU_PAS_2_BASE, \
+                             QEMU_PAS_2_SIZE, \
+                             GPT_GPI_ANY)
+
+#define QEMU_PAS_KERNEL     GPT_MAP_REGION_GRANULE(NS_DRAM0_BASE, \
+                            NS_DRAM0_SIZE, \
+                            GPT_GPI_NS)
+
+#define QEMU_PAS_REALM      GPT_MAP_REGION_GRANULE(QEMU_PAS_REALM_BASE, \
+                            QEMU_PAS_REALM_SIZE, \
+                            GPT_GPI_REALM)
+
+#endif
 
 /*
  * BL3-2 specific defines.
@@ -182,9 +229,15 @@
 #define NS_IMAGE_MAX_SIZE		(NS_DRAM0_SIZE - 0x20000000)
 
 #define PLAT_PHY_ADDR_SPACE_SIZE	(1ULL << 32)
-#define PLAT_VIRT_ADDR_SPACE_SIZE	(1ULL << 32)
-#define MAX_MMAP_REGIONS		11
-#define MAX_XLAT_TABLES			6
+/*
+ * Temporary fix
+ * Set the PLAT_VIRT_ADDR_SPACE_SIZE twice the PLAT_PHY_ADDR_SPACE_SIZE
+ * rmm_read_ns
+ */
+#define PLAT_VIRT_ADDR_SPACE_SIZE	(1ULL << 33)
+#define MAX_MMAP_REGIONS		14
+#define MAX_XLAT_TABLES			7
+#define PLAT_ARM_MMAP_ENTRIES	14
 #define MAX_IO_DEVICES			4
 #define MAX_IO_HANDLES			4
 
@@ -265,6 +318,12 @@
 #define PLAT_QEMU_DT_BASE		NS_DRAM0_BASE
 #define PLAT_QEMU_DT_MAX_SIZE		0x100000
 
+/*
+ * SMMU related constants
+ */
+#define PLAT_QEMU_SMMUV3_BASE          UL(0x09050000)
+#define PLAT_ARM_SMMUV3_ROOT_REG_OFFSET UL(0x20000)
+
 /*
  * System counter
  */
diff --git a/plat/qemu/qemu/platform.mk b/plat/qemu/qemu/platform.mk
index 6a877c3ff..4d327fc87 100644
--- a/plat/qemu/qemu/platform.mk
+++ b/plat/qemu/qemu/platform.mk
@@ -7,6 +7,11 @@
 # Use the GICv2 driver on QEMU by default
 QEMU_USE_GIC_DRIVER	:= QEMU_GICV2
 
+ifneq ($(ENABLE_RME), 0)
+QEMU_USE_GIC_DRIVER	:= QEMU_GICV3
+ENABLE_SPE_FOR_LOWER_ELS := 0
+endif
+
 ifeq (${ARM_ARCH_MAJOR},7)
 # ARMv7 Qemu support in trusted firmware expects the Cortex-A15 model.
 # Qemu Cortex-A15 model does not implement the virtualization extension.
@@ -50,11 +55,17 @@ endif
 
 PLAT_BL_COMMON_SOURCES	:=	${PLAT_QEMU_COMMON_PATH}/qemu_common.c			\
 				${PLAT_QEMU_COMMON_PATH}/qemu_console.c		  \
+				$(PLAT_QEMU_COMMON_PATH)/qemu_plat_attest_token.c	\
+				$(PLAT_QEMU_COMMON_PATH)/qemu_realm_attest_key.c	\
 				drivers/arm/pl011/${ARCH}/pl011_console.S
 
 include lib/xlat_tables_v2/xlat_tables.mk
 PLAT_BL_COMMON_SOURCES	+=	${XLAT_TABLES_LIB_SRCS}
 
+ifeq (${ENABLE_RME},1)
+	BL31_CPPFLAGS   +=  -DPLAT_XLAT_TABLES_DYNAMIC
+endif
+
 ifneq (${TRUSTED_BOARD_BOOT},0)
 
     AUTH_SOURCES	:=	drivers/auth/auth_mod.c			\
@@ -128,6 +139,7 @@ BL1_SOURCES		+=	drivers/io/io_semihosting.c		\
 				drivers/io/io_storage.c			\
 				drivers/io/io_fip.c			\
 				drivers/io/io_memmap.c			\
+				drivers/arm/smmu/smmu_v3.c		\
 				lib/semihosting/semihosting.c		\
 				lib/semihosting/${ARCH}/semihosting_call.S \
 				${PLAT_QEMU_COMMON_PATH}/qemu_io_storage.c		\
@@ -201,6 +213,7 @@ BL31_SOURCES		+=	lib/cpus/aarch64/aem_generic.S		\
 				plat/common/plat_psci_common.c		\
 				drivers/arm/pl061/pl061_gpio.c		\
 				drivers/gpio/gpio.c			\
+				drivers/arm/smmu/smmu_v3.c		\
 				${PLAT_QEMU_COMMON_PATH}/qemu_pm.c			\
 				${PLAT_QEMU_COMMON_PATH}/topology.c			\
 				${PLAT_QEMU_COMMON_PATH}/aarch64/plat_helpers.S	\
diff --git a/plat/qemu/qemu/trp/trp-qemu.mk b/plat/qemu/qemu/trp/trp-qemu.mk
new file mode 100644
index 000000000..2829ecbf2
--- /dev/null
+++ b/plat/qemu/qemu/trp/trp-qemu.mk
@@ -0,0 +1,5 @@
+# TRP source files specific to QEMU platform
+
+RMM_SOURCES	+= plat/qemu/common/aarch64/plat_helpers.S
+
+include plat/qemu/common/trp/qemu_trp.mk
diff --git a/services/std_svc/rmmd/rmmd_main.c b/services/std_svc/rmmd/rmmd_main.c
index 746419e9c..6c9030d23 100644
--- a/services/std_svc/rmmd/rmmd_main.c
+++ b/services/std_svc/rmmd/rmmd_main.c
@@ -119,11 +119,6 @@ static void manage_extensions_realm(cpu_context_t *ctx)
 	 * contexts are properly managed.
 	 */
 	sve_enable(ctx);
-#else
-	/*
-	 * Disable SVE and FPU in realm context when it is disabled for NS.
-	 */
-	sve_disable(ctx);
 #endif /* ENABLE_SVE_FOR_NS */
 }
 
diff --git a/services/std_svc/rmmd/trp/exception.S b/services/std_svc/rmmd/trp/exception.S
new file mode 100644
index 000000000..2c176da20
--- /dev/null
+++ b/services/std_svc/rmmd/trp/exception.S
@@ -0,0 +1,172 @@
+#include <arch.h>
+#include <asm_macros.S>
+#include "trp_private.h"
+
+    .global rmm_vector
+
+.macro unhandled_exception name
+	vector_entry \name
+    b do_panic
+	end_vector_entry \name
+.endm
+
+func do_panic
+loop:
+    wfi
+    b loop
+endfunc do_panic
+
+vector_base rmm_vector
+
+	/*
+	 * Current EL with SP0 : 0x0 - 0x200.
+	 */
+unhandled_exception sync_exception_sp_el0
+unhandled_exception irq_sp_el0
+unhandled_exception fiq_sp_el0
+unhandled_exception serror_sp_el0
+
+	/*
+	 * Current EL with SPx : 0x200 - 0x400.
+	 */
+vector_entry sync_exception_sp_elx
+    b sync_exception_vector_entry
+end_vector_entry sync_exception_sp_elx
+
+unhandled_exception irq_sp_elx
+unhandled_exception fiq_sp_elx
+unhandled_exception serror_sp_elx
+
+	/*
+	 * Lower EL using AArch64 : 0x400 - 0x600.
+	 */
+vector_entry sync_exception_aarch64
+    esb
+    stp x0, x1, [sp, #-16]
+    mov x0, #ARM_EXCEPTION_TRAP
+    ldr x1, [sp]
+    b __realm_exit
+end_vector_entry sync_exception_aarch64
+
+vector_entry irq_aarch64
+    esb
+    stp x0, x1, [sp, #-16]
+    mov x0, #ARM_EXCEPTION_IRQ
+    ldr x1, [sp]
+    b __realm_exit
+end_vector_entry irq_aarch64
+
+vector_entry fiq_aarch64
+    esb
+    stp x0, x1, [sp, #-16]
+    mov x0, #ARM_EXCEPTION_FIQ
+    ldr x1, [sp]
+    b __realm_exit
+end_vector_entry fiq_aarch64
+
+vector_entry serror_aarch64
+    esb
+    stp x0, x1, [sp, #-16]
+    mov x0, #ARM_EXCEPTION_EL1_SERROR
+    ldr x1, [sp]
+    b __realm_exit
+end_vector_entry serror_aarch64
+
+	/*
+	 * Lower EL using AArch32 : 0x600 - 0x800.
+	 */
+unhandled_exception sync_exception_aarch32
+unhandled_exception irq_aarch32
+unhandled_exception fiq_aarch32
+unhandled_exception serror_aarch32
+
+func sync_exception_vector_entry
+    mrs x0, esr_el2
+    ubfx x0, x0, #ESR_EC_SHIFT, #ESR_EC_LENGTH
+    cmp x0, #EC_DABORT_CUR_EL
+    b.ne 1f
+    bl rmm_data_abort_handler
+1:
+    b do_panic
+endfunc sync_exception_vector_entry
+
+.global __realm_enter
+
+/*
+ * uint64_t __realm_enter(rmm_rec_t *rec);
+ */
+func __realm_enter
+    //x0: rec
+    sub sp, sp, #112
+
+    # save RMM general purpose registers to stack
+    stp x0,  x18, [sp, #16 * 0]
+    stp x19, x20, [sp, #16 * 1]
+    stp x21, x22, [sp, #16 * 2]
+    stp x23, x24, [sp, #16 * 3]
+    stp x25, x26, [sp, #16 * 4]
+    stp x27, x28, [sp, #16 * 5]
+    stp x29, x30, [sp, #16 * 6]
+
+    # load realm general purpose registers from REC
+    ldp x2, x3, [x0, #16 * 1]
+    ldp x4, x5, [x0, #16 * 2]
+    ldp x6, x7, [x0, #16 * 3]
+    ldp x8, x9, [x0, #16 * 4]
+    ldp x10, x11, [x0, #16 * 5]
+    ldp x12, x13, [x0, #16 * 6]
+    ldp x14, x15, [x0, #16 * 7]
+    ldp x16, x17, [x0, #16 * 8]
+    ldp x18, x19, [x0, #16 * 9]
+    ldp x20, x21, [x0, #16 * 10]
+    ldp x22, x23, [x0, #16 * 11]
+    ldp x24, x25, [x0, #16 * 12]
+    ldp x26, x27, [x0, #16 * 13]
+    ldp x28, x29, [x0, #16 * 14]
+    ldr x30, [x0, #16 * 15]
+    # load x0 last
+    ldp x0, x1, [x0, #16 * 0]
+
+    eret
+endfunc __realm_enter
+
+func __realm_exit
+    // x0: return code
+    // x1: rec
+    // x2-x29,lr: rec regs
+    // rec x0-x1 on the stack
+
+    # save realm general purpose registers to REC
+    stp x2, x3, [x1, #16 * 1]
+    stp x4, x5, [x1, #16 * 2]
+    stp x6, x7, [x1, #16 * 3]
+    stp x8, x9, [x1, #16 * 4]
+    stp x10, x11, [x1, #16 * 5]
+    stp x12, x13, [x1, #16 * 6]
+    stp x14, x15, [x1, #16 * 7]
+    stp x16, x17, [x1, #16 * 8]
+    stp x18, x19, [x1, #16 * 9]
+    stp x20, x21, [x1, #16 * 10]
+    stp x22, x23, [x1, #16 * 11]
+    stp x24, x25, [x1, #16 * 12]
+    stp x26, x27, [x1, #16 * 13]
+    stp x28, x29, [x1, #16 * 14]
+    str x30, [x1, #16 * 15]
+
+    # save realm's x0-x1
+    ldp x2, x3, [sp, #-16]
+    stp x2, x3, [x1, #16 * 0]
+
+    # restore RMM general purpose registers from stack
+    ldr x18, [sp, #8]
+    ldp x19, x20, [sp, #16 * 1]
+    ldp x21, x22, [sp, #16 * 2]
+    ldp x23, x24, [sp, #16 * 3]
+    ldp x25, x26, [sp, #16 * 4]
+    ldp x27, x28, [sp, #16 * 5]
+    ldp x29, x30, [sp, #16 * 6]
+    add sp, sp, #112
+
+    # return as if from __realm_enter
+    ret
+endfunc __realm_exit
diff --git a/services/std_svc/rmmd/trp/granule.c b/services/std_svc/rmmd/trp/granule.c
new file mode 100644
index 000000000..3cac2cb07
--- /dev/null
+++ b/services/std_svc/rmmd/trp/granule.c
@@ -0,0 +1,272 @@
+#include <common/debug.h>
+#include <services/trp/platform_trp.h>
+#include <string.h>
+#include <inttypes.h>
+#include <platform_def.h>
+#include <plat/common/platform.h>
+#include <lib/libc/setjmp.h>
+#include "trp_private.h"
+#include "time.h"
+
+static uint8_t granule_state_table[RMM_NUM_GRANULES/2]; // 4 bits per granule, 2 granule per byte
+
+void rmm_granule_state_table_init (void)
+{
+	memset(granule_state_table, 0, sizeof(granule_state_table));
+}
+
+rmm_granule_state_e rmm_get_granule_state (rmm_addr_t addr)
+{
+	int granule_number = (addr >> RMM_GRANULE_SIZE2) & (RMM_NUM_GRANULES - 1);
+	int index = granule_number >> 1;
+	int shift = (granule_number & 1) << 2;
+	/*VERBOSE("rmm_get_granule_state 0x%llx. gn=0x%x, statex2=%x\n",
+			(unsigned long long)addr, granule_number,
+			granule_state_table[index]);*/
+	return (granule_state_table[index] >> shift) & 0xf;
+}
+
+void rmm_set_granule_state (rmm_addr_t addr, rmm_granule_state_e new_state)
+{
+	int granule_number = (addr >> RMM_GRANULE_SIZE2) & (RMM_NUM_GRANULES - 1);
+	int index = granule_number >> 1;
+	/*VERBOSE("rmm_set_granule_state 0x%llx %d. gn=0x%x, old-statex2=%x\n",
+			(unsigned long long)addr, new_state, granule_number,
+			granule_state_table[index]);*/
+	if (granule_number & 1)
+		granule_state_table[index] = (new_state << 4) | (granule_state_table[index] & 0xf);
+	else
+		granule_state_table[index] = (granule_state_table[index] & 0xf0) | new_state;
+}
+
+static jmp_buf rmm_jmp_buf[PLATFORM_CORE_COUNT];
+static bool rmm_jmp_ready[PLATFORM_CORE_COUNT]; /* rmm_jmp_ready is only for debug purpose */
+bool rmm_read_ns (void *rmm_dst, const void *ns_src, size_t size)
+{
+	assert(((uint64_t)ns_src & (RMM_GRANULE_SIZE - 1UL)) + size <= RMM_GRANULE_SIZE); // cannot read across granule boundary
+#ifdef PLAT_fvp
+	uint64_t ns_granule = ((uint64_t)ns_src) & ~(RMM_GRANULE_SIZE - 1UL);
+	rmm_remap_granule(ns_granule, MT_MEMORY | MT_RO | MT_NS);
+#endif
+	bool succeed;
+	unsigned int core_id = plat_my_core_pos();
+	rmm_jmp_ready[core_id] = true;
+	if (setjmp(rmm_jmp_buf[core_id])) {
+		succeed = false;
+	} else {
+#ifdef PLAT_qemu
+		memcpy(rmm_dst, ns_src + 0x100000000, size);
+#elif PLAT_fvp
+		memcpy(rmm_dst, ns_src , size);
+#endif
+		succeed = true;
+	}
+	rmm_jmp_ready[core_id] = false;
+#ifdef PLAT_fvp
+	rmm_undo_remap_granule(ns_granule);
+#endif
+	return succeed;
+}
+
+bool rmm_write_ns (void *ns_dst, const void *rmm_src, size_t size)
+{
+	assert(((uint64_t)ns_dst & (RMM_GRANULE_SIZE - 1UL)) + size <= RMM_GRANULE_SIZE); // cannot read across granule boundary
+#ifdef PLAT_fvp
+	uint64_t ns_granule = ((uint64_t)ns_dst) & ~(RMM_GRANULE_SIZE - 1UL);
+	rmm_remap_granule(ns_granule, MT_MEMORY | MT_RW | MT_NS);
+#endif
+
+	bool succeed;
+	unsigned int core_id = plat_my_core_pos();
+	rmm_jmp_ready[core_id] = true;
+	if (setjmp(rmm_jmp_buf[core_id])) {
+		succeed = false;
+	} else {
+#ifdef PLAT_qemu
+		memcpy(ns_dst + 0x100000000, rmm_src, size);
+#elif PLAT_fvp
+		memcpy(ns_dst , rmm_src, size);
+#endif
+		succeed = true;
+	}
+	rmm_jmp_ready[core_id] = false;
+#ifdef PLAT_fvp
+	rmm_undo_remap_granule(ns_granule);
+#endif
+	return succeed;
+}
+
+void rmm_data_abort_handler(void)
+{
+	uint64_t esr_el2 = read_esr_el2();
+	uint64_t far_el2 = read_far_el2();
+	uint64_t elr_el2 = read_elr_el2();
+	unsigned int core_id = plat_my_core_pos();
+
+	INFO("rmm_data_abort_handler: esr 0x%" PRIx64 " far 0x%" PRIx64 " elr 0x%" PRIx64 "\n",
+			esr_el2, far_el2, elr_el2);
+	assert(rmm_jmp_ready[core_id]);
+	longjmp(rmm_jmp_buf[core_id], 1);
+	__builtin_unreachable();
+}
+
+static rmm_rtt_entry_state_e rmm_get_rtte_state_internal (uint64_t rtte, int level)
+{
+	uint64_t outaddr = rtte & RTT_OA_MASK;
+	if (rtte & 1) { // valid
+		if (!outaddr) {
+			ERROR("rmm_get_rtte_state valid but no addr. rtte=%llx, level=%d\n",
+					(unsigned long long)rtte, level);
+			return ES_UNASSIGNED;
+		}
+		if ((rtte & 2) && level != RMM_RTT_PAGE_LEVEL)
+			return ES_TABLE;
+		if (level == RMM_RTT_BLOCK_LEVEL || level == RMM_RTT_PAGE_LEVEL)
+			return (rtte & (1UL << RTT_NS_BIT)) ? ES_VALID_NS : ES_VALID;
+	} else { // invalid
+		if (outaddr)
+			return ES_ASSIGNED;
+		return (rtte & (1UL << RTT_DESTROYED_BIT)) ? ES_DESTROYED : ES_UNASSIGNED;
+	}
+	ERROR("rmm_get_rtte_state unknown state. rtte=%llx, level=%d\n",
+			(unsigned long long)rtte, level);
+	return ES_UNASSIGNED;
+}
+
+rmm_rtt_entry_state_e rmm_get_rtte_state (const rmm_rtt_walk_result_t *walk_result)
+{
+	return rmm_get_rtte_state_internal(*(walk_result->rtte), walk_result->level);
+}
+
+uint64_t rmm_get_rtte_outaddr (const rmm_rtt_walk_result_t *walk_result)
+{
+	return *(walk_result->rtte) & RTT_OA_MASK;
+}
+
+/* only for page or block entry */
+void rmm_set_rtte (rmm_rtt_walk_result_t *walk_result, uint64_t out_addr, bool valid_bit)
+{
+	/* AF=1; SH=3; S2AP=3; MEMATTR=inner wb (s2fwb) */
+	const uint64_t PAGE_ATTR = (1UL << 10) | (3UL << 8) | (3UL << 6) | (6UL << 2);
+	uint64_t ls2b = walk_result->level == RMM_RTT_PAGE_LEVEL ? 2UL : 0UL;
+	*(walk_result->rtte) = PAGE_ATTR | (out_addr & RTT_OA_MASK) | ls2b | (valid_bit ? 1UL : 0UL);
+}
+
+void rmm_set_rtte_table (rmm_rtt_walk_result_t *walk_result, uint64_t child_rtt)
+{
+	*(walk_result->rtte) = (child_rtt & RTT_OA_MASK) | 3UL;
+}
+
+void rmm_set_rtte_valid (rmm_rtt_walk_result_t *walk_result, bool is_valid)
+{
+	if (is_valid)
+		*(walk_result->rtte) |= 1UL;
+	else
+		*(walk_result->rtte) &= ~1UL;
+}
+
+void rmm_set_rtte_destroyed (rmm_rtt_walk_result_t *walk_result)
+{
+	*(walk_result->rtte) = 1UL << RTT_DESTROYED_BIT;
+}
+
+/* copy MemAttr[2:5], S2AP[6:7], SH[8:9], OA[12:47]
+ * set VALID, NS */
+void rmm_set_rtte_ns (rmm_rtt_walk_result_t *walk_result, uint64_t ns_rtte)
+{
+	uint64_t rtte = 1 | (1UL << RTT_NS_BIT); // ns, valid
+	rtte |= ns_rtte & 0xfffffffff3fcUL;
+	*(walk_result->rtte) = rtte;
+}
+
+/* LEVEL_WIDTH: How big does one RTT cover in bits? level 3 covers 21 bits (2MB) */
+#define LEVEL_WIDTH(level) (12 + 9 * (4 - (level)))
+
+/* precon:
+ *   rtt_level_start <= target_level <= 3
+ *   target_addr < 2^ipa_width */
+void rmm_rtt_walk (rmm_rtt_walk_result_t *result, const rmm_realm_t *rd, uint64_t target_addr, int target_level)
+{
+	int curr_level = rd->rtt_level_start;
+	/* ARM supports multiple consecutive root page tables.
+	 * <root_index> is which one to use. */
+	const uint64_t root_index = (target_addr >> LEVEL_WIDTH(curr_level));
+	assert(root_index < rd->rtt_num_start);
+	uint64_t curr_rtt = rd->rtt_base + (root_index << 12);
+
+	while (1) {
+		unsigned int index = (target_addr >> LEVEL_WIDTH(curr_level + 1)) & 0x1ff;
+		/*INFO("target_addr=0x%llx, curr_level=%d, root_index=0x%llx, index=%u\n",
+				(unsigned long long)target_addr, curr_level, (unsigned long long)root_index, index);*/
+		uint64_t rtte = ((uint64_t *)curr_rtt)[index];
+		if ((rtte & 3) != 3 || // invalid or block
+				curr_level == target_level) { // level reached
+			result->rtt_addr = curr_rtt;
+			result->rtte = &((uint64_t *)curr_rtt)[index];
+			result->level = curr_level;
+			return;
+		}
+		uint64_t out_addr = rtte & RTT_OA_MASK;
+		if (!out_addr) {
+			ERROR("valid rtte but no outaddr curr_rtt=0x%llx, rtte=0x%llx\n",
+					(unsigned long long)curr_rtt, (unsigned long long)rtte);
+			return;
+		}
+		curr_rtt = out_addr;
+		curr_level ++;
+	}
+}
+
+/* Fold the child RTT pointed by parent_rtte.
+ * precon:
+ *   parent_rtte is ES_TABLE
+ * postcon:
+ *   parent_rtte address and state is set to the folded result
+ *   child RTT state is unchanged. i.e. still GS_RTT */
+bool rmm_rtt_fold(rmm_rtt_walk_result_t *parent_rtte)
+{
+	assert(rmm_get_rtte_state(parent_rtte) == ES_TABLE);
+	uint64_t child_rtt = rmm_get_rtte_outaddr(parent_rtte);
+	int level = parent_rtte->level + 1;
+	rmm_rtt_entry_state_e parent_state = rmm_get_rtte_state_internal(*(uint64_t *)child_rtt, level);
+	bool aligned = false;
+	uint64_t parent_oa = 0;
+	if (level == RMM_RTT_PAGE_LEVEL) {
+		parent_oa = *(uint64_t *)child_rtt & RTT_OA_MASK;
+		if (rmm_is_addr_rtt_level_aligned(parent_oa, RMM_RTT_BLOCK_LEVEL))
+			aligned = true;
+	}
+	for (int i = 1; i < RMM_GRANULE_SIZE / 8; i ++) {
+		uint64_t rtte = ((uint64_t *)child_rtt)[i];
+		rmm_rtt_entry_state_e state = rmm_get_rtte_state_internal(rtte, level);
+		if (state != parent_state) {
+			if (parent_state == ES_UNASSIGNED && state == ES_DESTROYED) {
+				parent_state = ES_DESTROYED;
+			} else if (parent_state == ES_DESTROYED && state == ES_UNASSIGNED) {
+			} else {
+				INFO("rmm_rtt_fold not foldable: ps=%d, cs=%d, crtt=0x%llx\n",
+						parent_state, state, (unsigned long long)child_rtt);
+				return false;
+			}
+		}
+		if (aligned) {
+			uint64_t oa = rtte & RTT_OA_MASK;
+			if (oa != parent_oa + (i << 12))
+				aligned = false;
+		}
+	}
+	if (parent_state == ES_UNASSIGNED) {
+		*(parent_rtte->rtte) = 0;
+	} else if (parent_state == ES_DESTROYED) {
+		*(parent_rtte->rtte) = 1UL << RTT_DESTROYED_BIT;
+	} else if (aligned && (parent_state == ES_ASSIGNED ||
+				parent_state == ES_VALID ||
+				parent_state == ES_VALID_NS)) {
+		*(parent_rtte->rtte) = *(uint64_t *)child_rtt;
+	} else {
+		INFO("rmm_rtt_fold not foldable: ps=%d, aligned=%d, crtt=0x%llx\n",
+				parent_state, aligned, (unsigned long long)child_rtt);
+		return false;
+	}
+	return true;
+}
diff --git a/services/std_svc/rmmd/trp/linker.lds b/services/std_svc/rmmd/trp/linker.lds
index 2b7f38333..c45580f0b 100644
--- a/services/std_svc/rmmd/trp/linker.lds
+++ b/services/std_svc/rmmd/trp/linker.lds
@@ -24,15 +24,22 @@ SECTIONS
 	. = RMM_BASE;
 
 	.text : {
+		__TEXT_START__ = .;
 		*(.head.text)
 		. = ALIGN(8);
 		*(.text*)
+		*(.vectors)
+		. = ALIGN(PAGE_SIZE_4K);
+		__TEXT_END__ = .;
 	} >RAM
 
 	. = ALIGN(PAGE_SIZE_4K);
 
 	.rodata : {
+		__RODATA_START__ = .;
 		*(.rodata*)
+		. = ALIGN(PAGE_SIZE_4K);
+		__RODATA_END__ = .;
 	} >RAM
 
 	. = ALIGN(PAGE_SIZE_4K);
diff --git a/services/std_svc/rmmd/trp/rmi.h b/services/std_svc/rmmd/trp/rmi.h
new file mode 100644
index 000000000..a83e9eeb5
--- /dev/null
+++ b/services/std_svc/rmmd/trp/rmi.h
@@ -0,0 +1,65 @@
+#ifndef __RMM_RMI_H
+#define __RMM_RMI_H
+#include <stdint.h>
+
+typedef struct rmi_realm_params {
+	uint64_t par_base;
+	uint64_t par_size;
+	uint64_t rtt_base;
+	uint64_t measurement_algo;
+	uint64_t features_0;
+	int64_t rtt_level_start;
+	uint32_t rtt_num_start;
+	uint32_t vmid;
+} rmi_realm_params_t;
+
+typedef struct rmi_rec_params {
+	uint64_t gprs[8];
+	uint64_t pc;
+	uint64_t flags;
+	uint64_t aux[16];
+} rmi_rec_params_t;
+
+#define RMI_DISPOSE_ACCEPT 0
+#define RMI_DISPOSE_REJECT 1
+
+typedef struct rmi_rec_entry {
+	uint64_t gprs[7];
+	uint64_t is_emulated_mmio;
+	uint64_t emulated_read_value;
+	uint64_t dispose_response;
+	uint64_t gicv3_lrs[16];
+	uint64_t gicv3_hcr;
+} rmi_rec_entry_t;
+
+#define RMI_EXIT_SYNC    0
+#define RMI_EXIT_IRQ     1
+#define RMI_EXIT_FIQ     2
+#define RMI_EXIT_PSCI    3
+#define RMI_EXIT_DISPOSE 4
+
+typedef struct rmi_rec_exit {
+	uint64_t reason;
+	uint64_t esr;
+	uint64_t far_;
+	uint64_t hpfar;
+	uint64_t emulated_write_value;
+	uint64_t gprs[7];
+	uint64_t dispose_base;
+	uint64_t dispose_size;
+	uint64_t gicv3_vmcr;
+	uint64_t gicv3_misr;
+	uint64_t cntv_ctl;
+	uint64_t cntv_cval;
+	uint64_t cntp_ctl;
+	uint64_t cntp_cval;
+	uint64_t gicv3_lrs[16];
+	uint64_t gicv3_hcr;
+} rmi_rec_exit_t;
+
+typedef struct rmi_rec_run {
+	rmi_rec_entry_t entry;
+	rmi_rec_exit_t exit;
+} rmi_rec_run_t;
+
+#endif
diff --git a/services/std_svc/rmmd/trp/time.c b/services/std_svc/rmmd/trp/time.c
new file mode 100644
index 000000000..7a682ccba
--- /dev/null
+++ b/services/std_svc/rmmd/trp/time.c
@@ -0,0 +1,11 @@
+#include <inttypes.h>
+#include <plat/common/platform.h>
+
+uint64_t get_timer_value_us(uint64_t start)
+{
+	uint64_t cntpct;
+
+	isb();
+	cntpct = read_cntpct_el0();
+	return (cntpct * 1000000ULL / read_cntfrq_el0() - start);
+}
\ No newline at end of file
diff --git a/services/std_svc/rmmd/trp/time.h b/services/std_svc/rmmd/trp/time.h
new file mode 100644
index 000000000..4f8b2074e
--- /dev/null
+++ b/services/std_svc/rmmd/trp/time.h
@@ -0,0 +1,15 @@
+#ifndef __RMM_TIME_H
+#define __RMM_TIME_H
+#include <stdint.h>
+
+uint64_t get_timer_value_us(uint64_t start);
+
+#define MEASURE_TIME_START(mark)                    \
+    uint64_t start_time_##mark = get_timer_value_us(0);
+
+#define MEASURE_TIME_STOP(mark)                    \
+    uint64_t elapse_time_##mark = get_timer_value_us(start_time_##mark); \
+	printf("Function: %s  measure time interval(%d): %ld.%ld MS\n", __func__, mark, elapse_time_##mark/1000, elapse_time_##mark%1000);
+
+
+#endif
diff --git a/services/std_svc/rmmd/trp/trp.mk b/services/std_svc/rmmd/trp/trp.mk
index a4f6e03e0..a15322f7a 100644
--- a/services/std_svc/rmmd/trp/trp.mk
+++ b/services/std_svc/rmmd/trp/trp.mk
@@ -5,10 +5,21 @@
 #
 
 RMM_SOURCES		+=	services/std_svc/rmmd/trp/trp_entry.S	\
-				services/std_svc/rmmd/trp/trp_main.c
+				services/std_svc/rmmd/trp/exception.S	\
+				services/std_svc/rmmd/trp/trp_main.c \
+				services/std_svc/rmmd/trp/time.c \
+				services/std_svc/rmmd/trp/granule.c	\
+				services/std_svc/rmmd/trp/vgic-v3.c	\
+				lib/locks/exclusive/aarch64/spinlock.S \
+				lib/el3_runtime/aarch64/context.S
 
 RMM_LINKERFILE		:=	services/std_svc/rmmd/trp/linker.lds
 
+RMM_CPPFLAGS   +=      -DPLAT_XLAT_TABLES_DYNAMIC
+
+# Save & restore during REC_ENTER so that Realm can access timer registers
+NS_TIMER_SWITCH     :=  1
+
 # Include the platform-specific TRP Makefile
 # If no platform-specific TRP Makefile exists, it means TRP is not supported
 # on this platform.
diff --git a/services/std_svc/rmmd/trp/trp_entry.S b/services/std_svc/rmmd/trp/trp_entry.S
index 1b03c9fbc..efba0b546 100644
--- a/services/std_svc/rmmd/trp/trp_entry.S
+++ b/services/std_svc/rmmd/trp/trp_entry.S
@@ -26,6 +26,29 @@
 	smc	#0
 	.endm
 
+	.macro __init_el2_gicv3
+	mrs x0, ICC_SRE_EL2
+	orr x0, x0, #ICC_SRE_EL2_SRE 		//Set ICC_SRE_EL2.SRE==1
+	bic x0, x0, #ICC_SRE_EL2_ENABLE		//Prevent the guest from touching the GIC
+	msr ICC_SRE_EL2, x0
+	isb
+	.endm
+
+	.macro __init_el2_timers
+	mov x0, #3                          //Enable EL1 physical timers
+	msr cnthctl_el2, x0
+	msr cntvoff_el2, xzr
+	.endm
+
+	.macro __init_el2_hstr              //Disable CP15 traps to EL2
+	msr hstr_el2, xzr
+	.endm
+
+	.macro __init_el2_cptr
+	mov x0, #0x33ff
+	msr cptr_el2, x0                    //Disable copro. traps to EL2
+	.endm
+
 	/* ---------------------------------------------
 	 * Entry point for TRP
 	 * ---------------------------------------------
@@ -57,7 +80,16 @@ trp_head:
 	bl	trp_setup
 
 	bl	trp_main
+
 warm_boot:
+	/* Set the exception vectors */
+	adr x0, rmm_vector
+	msr vbar_el2, x0
+
+	bl 	trp_enable_mmu
+	__init_el2_gicv3
+	__init_el2_timers
+	__init_el2_hstr
 	mov_imm	x0, RMMD_RMI_REQ_COMPLETE
 	mov	x1, xzr
 	smc	#0
diff --git a/services/std_svc/rmmd/trp/trp_main.c b/services/std_svc/rmmd/trp/trp_main.c
index 2e3f07634..152c43aa5 100644
--- a/services/std_svc/rmmd/trp/trp_main.c
+++ b/services/std_svc/rmmd/trp/trp_main.c
@@ -4,20 +4,32 @@
  * SPDX-License-Identifier: BSD-3-Clause
  */
 
-
+#include <arch.h>
+#include <inttypes.h>
 #include <common/debug.h>
 #include <plat/common/platform.h>
 #include <services/rmmd_svc.h>
 #include <services/trp/platform_trp.h>
+#include <lib/xlat_tables/xlat_tables_v2.h>
+#include <lib/spinlock.h>
 
 #include <platform_def.h>
 #include "trp_private.h"
+#include "rmi.h"
+#include "vgic-v3.h"
+#include "time.h"
 
 /*******************************************************************************
  * Per cpu data structure to populate parameters for an SMC in C code and use
  * a pointer to this structure in assembler code to populate x0-x7
  ******************************************************************************/
 static trp_args_t trp_smc_args[PLATFORM_CORE_COUNT];
+static spinlock_t rmm_lock;
+
+static const unsigned int pa_range_bits_arr[] = {
+	PARANGE_0000, PARANGE_0001, PARANGE_0010, PARANGE_0011, PARANGE_0100,
+	PARANGE_0101, PARANGE_0110
+};
 
 /*******************************************************************************
  * Set the arguments for SMC call
@@ -59,16 +71,23 @@ void trp_setup(void)
 {
 	/* Perform early platform-specific setup */
 	trp_early_platform_setup();
+	trp_plat_arch_setup();
 }
 
 /* Main function for TRP */
 void trp_main(void)
 {
-	NOTICE("TRP: %s\n", version_string);
-	NOTICE("TRP: %s\n", build_message);
-	INFO("TRP: Memory base : 0x%lx\n", (unsigned long)RMM_BASE);
-	INFO("TRP: Total size : 0x%lx bytes\n", (unsigned long)(RMM_END
-								- RMM_BASE));
+	rmm_notice("TRP: %s\n", version_string);
+	rmm_notice("TRP: %s\n", build_message);
+	rmm_info("TRP: Memory base : 0x%lx\n", (unsigned long)RMM_BASE);
+	rmm_info("TRP: Total size : 0x%lx bytes\n", (unsigned long)(RMM_END
+			- RMM_BASE));
+}
+
+void trp_enable_mmu(void)
+{
+	int linear_id = plat_my_core_pos();
+	trp_plat_arch_enable_mmu(linear_id);
 }
 
 /*******************************************************************************
@@ -76,7 +95,7 @@ void trp_main(void)
  ******************************************************************************/
 static trp_args_t *trp_ret_rmi_version(void)
 {
-	VERBOSE("RMM version is %u.%u\n", RMI_ABI_VERSION_MAJOR,
+	rmm_verbose("RMM version is %u.%u\n", RMI_ABI_VERSION_MAJOR,
 					  RMI_ABI_VERSION_MINOR);
 	return set_smc_args(RMMD_RMI_REQ_COMPLETE, RMI_ABI_VERSION,
 			    0, 0, 0, 0, 0, 0);
@@ -89,13 +108,22 @@ static trp_args_t *trp_asc_mark_realm(unsigned long long x1)
 {
 	unsigned long long ret;
 
-	VERBOSE("Delegating granule 0x%llx\n", x1);
+	rmm_verbose("Delegating granule 0x%llx\n", x1);
+	//TODO: how to check Granule(addr).pas != NS
+	if (!rmm_assert_granule_state(x1, GS_UNDELEGATED)) {
+		rmm_info("Delegating 0x%llx with state %d\n", x1, rmm_get_granule_state(x1));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
 	ret = trp_smc(set_smc_args(RMMD_GTSI_DELEGATE, x1, 0, 0, 0, 0, 0, 0));
 
 	if (ret != 0ULL) {
 		ERROR("Granule transition from NON-SECURE type to REALM type "
-			"failed 0x%llx\n", ret);
+		      "failed 0x%llx\n", ret);
 	}
+	rmm_set_granule_state(x1, GS_DELEGATED);
+out:
 	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
 }
 
@@ -106,20 +134,1406 @@ static trp_args_t *trp_asc_mark_nonsecure(unsigned long long x1)
 {
 	unsigned long long ret;
 
-	VERBOSE("Undelegating granule 0x%llx\n", x1);
+	rmm_verbose("Undelegating granule 0x%llx\n", x1);
+	if (!rmm_assert_granule_state(x1, GS_DELEGATED)) {
+		rmm_info("Undelegating 0x%llx with state %d\n", x1, rmm_get_granule_state(x1));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	memset((void *)x1, 0, RMM_GRANULE_SIZE);
+
 	ret = trp_smc(set_smc_args(RMMD_GTSI_UNDELEGATE, x1, 0, 0, 0, 0, 0, 0));
 
 	if (ret != 0ULL) {
-		ERROR("Granule transition from REALM type to NON-SECURE type "
+		rmm_err("Granule transition from REALM type to NON-SECURE type "
 			"failed 0x%llx\n", ret);
 	}
+	rmm_set_granule_state(x1, GS_UNDELEGATED);
+out:
 	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
 }
 
+static bool rmm_is_feature_valid(int index, uint64_t feature)
+{
+	switch (index) {
+	case 0:
+		if (feature & 0x100) {
+			/* do not support FEAT_LPA2 */
+			return false;
+		}
+		uint8_t s2sz;
+		uint64_t pa_range = read_id_aa64mmfr0_el1() &
+				    ID_AA64MMFR0_EL1_PARANGE_MASK;
+		s2sz = feature & 0xff;
+		if (s2sz <= pa_range_bits_arr[pa_range]) {
+			return true;
+		}
+		return false;
+	default:
+		assert(0);
+	}
+
+	return false;
+}
+
+static inline bool rmm_is_rtt_params_valid(int64_t rtt_level_start,
+		uint64_t rtt_num_start,
+		unsigned int ipa_width)
+{
+	/* FEAT_LPA2 is not implemented, yet, so no rtt_level_start = -1. */
+	if (rtt_level_start < 0 ||
+	    rtt_level_start > RMM_RTT_PAGE_LEVEL) {
+		rmm_info("invalid rtt_level_start %d\n", (int)rtt_level_start);
+		return false;
+	}
+	/* Level 3 RTT covers 21 bits, i.e. 2MB.
+	 * Level 2 RTT covers 30 bits, i.e. 1GB.
+	 * <root_bits> is how many LSB address bit does one starting level RTT cover. */
+	int root_bits = 12 + (4 - rtt_level_start) * 9;
+	if (ipa_width + 9 < root_bits) {
+		rmm_info("rtt_level_start %d is too small for ipa_width %u. Don't need such a tall tree.\n",
+			 (int)rtt_level_start, ipa_width);
+		return false;
+	}
+	if (ipa_width > root_bits) { // need more than one starting level RTT
+		if (rtt_num_start != (1 << (ipa_width - root_bits))) {
+			rmm_info("rtt_num_start %llx should be %llx\n",
+				 (unsigned long long)rtt_num_start,
+				 1ULL << (ipa_width - root_bits));
+			return false;
+		}
+	}
+	return true;
+}
+
+#define VMID_LIMIT 256
+static bool rmm_vmid_used[VMID_LIMIT];
+static unsigned int rmm_last_vmid = 0;
+
+/* return 0 if no VMID available */
+static unsigned int get_vmid (void)
+{
+	for (int i = 0; i < VMID_LIMIT; i ++) {
+		rmm_last_vmid = (rmm_last_vmid + 1) % VMID_LIMIT;
+		if (rmm_last_vmid == 0) // vmid0 is reserved for RMM
+			continue;
+		if (!rmm_vmid_used[rmm_last_vmid]) {
+			rmm_vmid_used[rmm_last_vmid] = true;
+			return rmm_last_vmid;
+		}
+	}
+	return 0;
+}
+
+static void put_vmid (unsigned int vmid)
+{
+	rmm_vmid_used[vmid] = false;
+}
+
+static trp_args_t *trp_create_realm (rmm_addr_t rd, rmm_addr_t params)
+{
+	unsigned long long ret;
+	rmi_realm_params_t realm_params;
+	unsigned int ipa_width;
+
+	spin_lock(&rmm_lock);
+	rmm_verbose("create realm 0x%" PRIx64 " 0x%" PRIx64 "\n", rd, params);
+
+	if (!rmm_assert_granule_state(params, GS_UNDELEGATED)) {
+		rmm_info("create realm param 0x%" PRIx64 " with state %d\n", params,
+			 rmm_get_granule_state(params));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(rd, GS_DELEGATED)) {
+		rmm_info("create realm rd 0x%" PRIx64 " with state %d\n", rd,
+			 rmm_get_granule_state(rd));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	/* copy the params from NS to realm */
+	if (!rmm_read_ns(&realm_params, (void *)params, sizeof(realm_params))) {
+		rmm_info("read NS realm params failed: rd 0x%" PRIx64 " params 0x%" PRIx64 "\n", rd, params);
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	if (!rmm_is_feature_valid(0, realm_params.features_0)) {
+		ret = RMI_ERROR_MEMORY;
+		goto out;
+	}
+	ipa_width = realm_params.features_0 & 0xff;
+
+	if (!rmm_is_rtt_params_valid(realm_params.rtt_level_start,
+				     realm_params.rtt_num_start, ipa_width)) {
+		ret = RMI_ERROR_MEMORY;
+		goto out;
+	}
+
+	if (realm_params.par_base >= 1UL << ipa_width ||
+	    realm_params.par_size >= 1UL << ipa_width ||
+	    realm_params.par_base + realm_params.par_size >= 1UL << ipa_width) {
+		rmm_info("create realm ipa_width=%u, par_base=0x%" PRIx64 ", par_size=0x%" PRIx64 "\n",
+			 ipa_width, realm_params.par_base,
+			 realm_params.par_size);
+		ret = RMI_ERROR_MEMORY;
+		goto out;
+	}
+
+	for (int i = 0; i < realm_params.rtt_num_start; i++) {
+		if (!rmm_assert_granule_state(realm_params.rtt_base + RMM_GRANULE_SIZE * i, GS_DELEGATED)) {
+			rmm_info("create realm rtt_base 0x%" PRIx64 " with state %d\n",
+				 realm_params.rtt_base + RMM_GRANULE_SIZE * i,
+				 rmm_get_granule_state(realm_params.rtt_base + RMM_GRANULE_SIZE * i));
+			ret = RMI_ERROR_MEMORY;
+			goto out;
+		}
+	}
+
+	unsigned int vmid = get_vmid();
+	if (vmid == 0) {
+		rmm_info("create realm no available VMID\n");
+		ret = RMI_ERROR_INTERNAL;
+		goto out;
+	}
+
+	rmm_realm_t *rdp = (rmm_realm_t *)rd;
+	rdp->par_base = realm_params.par_base;
+	rdp->par_size = realm_params.par_size;
+	rdp->ipa_width = ipa_width;
+	rdp->rec_index = 0;
+	rdp->rec_count = 0;
+	rdp->rtt_base = realm_params.rtt_base;
+	rdp->rtt_level_start = realm_params.rtt_level_start;
+	rdp->rtt_num_start = realm_params.rtt_num_start;
+	rdp->vmid = vmid;
+	rdp->state = RS_NEW;
+
+	memset((void *)rdp->rtt_base, 0, RMM_GRANULE_SIZE * rdp->rtt_num_start);
+	for (int i = 0; i < realm_params.rtt_num_start; i ++)
+		rmm_set_granule_state(realm_params.rtt_base + RMM_GRANULE_SIZE * i, GS_RTT);
+	rmm_set_granule_state(rd, GS_RD);
+	ret = 0;
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static inline bool is_rtte_ready_to_destroy (uint64_t rtte)
+{
+	if (rtte & 3UL) // TABLE
+		return false;
+	uint64_t outaddr = rtte & RTT_OA_MASK;
+	if (!outaddr) // UNASSIGNED, DESTROYED
+		return true;
+	return rtte & (1UL << RTT_NS_BIT);
+}
+
+static trp_args_t *trp_destroy_realm (unsigned long long rd)
+{
+	unsigned long long ret;
+
+	spin_lock(&rmm_lock);
+	rmm_verbose("destroy realm 0x%llx\n", rd);
+
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		rmm_info("destroy realm rd 0x%llx with state %d\n", rd, rmm_get_granule_state(rd));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	const rmm_realm_t *prd = (const rmm_realm_t *)rd;
+
+	if (prd->rec_count) {
+		ret = RMI_ERROR_IN_USE;
+		goto out;
+	}
+
+	/* Make sure all RTTE in all starting level RTT is one of
+	 * UNASSIGNED, DESTROYED, VALID_NS.
+	 * I.e. not pointing to DATA or RTT*/
+	for (int i = 0; i < prd->rtt_num_start; i ++) {
+		uint64_t rtt = prd->rtt_base + i * RMM_GRANULE_SIZE;
+		if (!rmm_assert_granule_state(rtt, GS_RTT)) {
+			rmm_err("destroy realm but starting rtt 0x%llx is %d\n",
+				(unsigned long long)rtt, rmm_get_granule_state(rtt));
+			ret = RMI_ERROR_INTERNAL;
+			goto out;
+		}
+		for (int j = 0; j < RMM_GRANULE_SIZE / sizeof(uint64_t); j ++) {
+			uint64_t rtte = ((uint64_t *)rtt)[j];
+			if (!is_rtte_ready_to_destroy(rtte)) {
+				rmm_info("destroy realm but RTTE 0x%llx not ready to go\n", (unsigned long long)rtte);
+				ret = RMI_ERROR_IN_USE;
+				goto out;
+			}
+		}
+	}
+
+	put_vmid(prd->vmid);
+	for (int i = 0; i < prd->rtt_num_start; i ++) {
+		uint64_t rtt = prd->rtt_base + i * RMM_GRANULE_SIZE;
+		rmm_set_granule_state(rtt, GS_DELEGATED);
+	}
+	rmm_set_granule_state(rd, GS_DELEGATED);
+
+	ret = 0;
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_activate_realm (uint64_t rd)
+{
+	uint64_t ret;
+	spin_lock(&rmm_lock);
+	rmm_verbose("activate realm 0x%" PRIx64 "\n", rd);
+
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		rmm_info("activate realm 0x%" PRIx64 " with state %d\n", rd,
+			 rmm_get_granule_state(rd));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_realm_t *prd = (rmm_realm_t *)rd;
+	if (prd->state != RS_NEW) {
+		rmm_info("activate realm rd.state=%d\n", prd->state);
+		ret = RMI_ERROR_REALM_STATE;
+		goto out;
+	}
+
+	prd->state = RS_ACTIVE;
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static inline uint64_t mpidr2recindex (uint64_t mpidr)
+{
+	uint64_t aff3 = (mpidr >> 12UL) & 0x0ff00000;
+	uint64_t aff12 = (mpidr >> 4UL) & 0x000ffff0;
+	uint64_t aff0 = mpidr & 0xf;
+	return aff3 | aff12 | aff0;
+}
+
+/* input: mpidr parameter of REC_CREATE
+ * output: mpidr_el1 register */
+static inline uint64_t mpidr2reg(uint64_t mpidr)
+{
+	const uint64_t U = 0;
+	const uint64_t MT = 0;
+	const uint64_t RES1 = 0x80000000UL;
+	uint64_t aff3 = (mpidr >> 32UL) & 0xffUL;
+	uint64_t aff012 = mpidr & 0x00ffffffUL;
+	return aff3 | aff012 | RES1 | (U << 30UL) | (MT << 24UL);
+}
+
+static trp_args_t *trp_rec_create (uint64_t rec, uint64_t rd, uint64_t mpidr, uint64_t params)
+{
+	uint64_t ret;
+	spin_lock(&rmm_lock);
+	rmm_verbose("create REC 0x%" PRIx64 " 0x%" PRIx64 " 0x%" PRIx64 " 0x%" PRIx64 "\n",
+		    rec, rd, mpidr, params);
+
+	if (!rmm_assert_granule_state(params, GS_UNDELEGATED)) {
+		rmm_info("create rec param 0x%" PRIx64 " with state %d\n", params,
+			 rmm_get_granule_state(params));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		rmm_info("create rec rd 0x%" PRIx64 " with state %d\n", rd,
+			 rmm_get_granule_state(rd));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(rec, GS_DELEGATED)) {
+		rmm_info("create rec rec 0x%" PRIx64 " with state %d\n", rec,
+			 rmm_get_granule_state(rec));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_realm_t *prd = (rmm_realm_t *)rd;
+	if (prd->state != RS_NEW) {
+		rmm_info("create rec rd.state=%d\n", prd->state);
+		ret = RMI_ERROR_REALM_STATE;
+		goto out;
+	}
+	if (mpidr2recindex(mpidr) != prd->rec_index) {
+		rmm_info("create rec mpidr=0x%" PRIx64 ", rec_index=0x%" PRIx64 "\n",
+			 mpidr, prd->rec_index);
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmi_rec_params_t rec_params;
+	if (!rmm_read_ns(&rec_params, (void *)params, sizeof(rec_params))) {
+		rmm_info("read NS rec params failed: params 0x%" PRIx64 "\n", params);
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	prd->rec_index ++;
+	prd->rec_count ++;
+	rmm_set_granule_state(rec, GS_REC);
+	rmm_rec_t *prec = (rmm_rec_t *)rec;
+	prec->owner_rd = prd;
+	prec->state_running = false;
+	prec->runnable = (rec_params.flags & 1) == 1; // TODO check RES0 bits?
+	for (int i = 0; i < 8; i ++)
+		prec->gprs[i] = rec_params.gprs[i];
+	for (int i = 8; i < 32; i ++)
+		prec->gprs[i] = 0;
+	prec->pc = rec_params.pc;
+	prec->dispose_size = 0;
+
+	prec->enter_reason = REC_ENTER_REASON_FIRST_RUN;
+
+	// Set sctlr_el1, spsr_el2, hcr_el2, vtcr_el2 based on current observation
+	memset((void*)(&prec->sysregs), 0, sizeof(rmm_system_registers_t));
+
+	write_ctx_reg(&(prec->sysregs.el1regs), CTX_SCTLR_EL1,
+		      SCTLR_SA_BIT | SCTLR_SA0_BIT | SCTLR_CP15BEN_BIT | SCTLR_nAA_BIT
+		      | SCTLR_NTWI_BIT | SCTLR_NTWE_BIT | SCTLR_EIS_BIT | SCTLR_SPAN_BIT);
+
+	/* Set SPSR_EL2, set PSTATE.EL and PSTATE.SP on executing an exception
+	 * return operation in EL2. At present we set PSTATE.EL = EL1,
+	 * PSTATE.SP=SP_ELx, need to reconsider in future
+	 */
+	prec->sysregs.spsr_el2 = SPSR_64(MODE_EL1, MODE_SP_ELX, DAIF_DBG_BIT | DAIF_ABT_BIT | DAIF_IRQ_BIT | DAIF_FIQ_BIT);
+
+	/* Set HCR_EL2
+	 * HCR_FWB_BIT: Enable stage 2 FWB
+	 * HCR_RW_BIT: Register width control for lower exception. RW=1: EL1 is AArch64
+	 * HCR_TSC_BIT: Trap SMC instruction. TSC=1: SMC instruction executed in NS_EL1 is trapped to EL2
+	 * HCR_BSU: Barrier shareability
+	 * HCR_VM_BIT:  Enable second stage of translation. VM=1: Enable second stage translation
+	 */
+	prec->sysregs.hcr_el2 = HCR_VM_BIT| HCR_SWIO_BIT | HCR_PTW_BIT |
+				HCR_FMO_BIT | HCR_IMO_BIT | HCR_AMO_BIT |
+				HCR_FB_BIT | HCR_BSU_IS_VAL | HCR_TWI_BIT | HCR_TWE_BIT |
+				HCR_TSC_BIT | HCR_RW_BIT | HCR_FWB_BIT;
+
+	/* Set VTCR_EL2, need to reconfigure according to stage2 configuration
+	 * VTCR_SL0: Starting level of the stage 2 translation lookup
+	 * VTCR_T0SZ: The size offset of the memory region addressed by VTTBR_EL2
+	 * VTCR_PS: Physical address size of the second stage translation
+	 */
+	prec->sysregs.vtcr_el2 = VTCR_INIT_FLAGS_EL2 |
+				 VTCR_SL0_EL2(prd->rtt_level_start) | VTCR_T0SZ_EL2(prd->ipa_width) |
+				 VTCR_PS_32BIT_EL2;
+
+	prec->sysregs.vttbr_el2 = prd->rtt_base | ((uint64_t)prd->vmid << 48);
+	prec->sysregs.vmpidr_el2 = mpidr2reg(mpidr);
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_rec_destroy (uint64_t rec)
+{
+	uint64_t ret;
+	spin_lock(&rmm_lock);
+	rmm_verbose("destroy REC 0x%" PRIx64 "\n", rec);
+
+	if (!rmm_assert_granule_state(rec, GS_REC)) {
+		rmm_info("destroy rec 0x%" PRIx64 " with state %d\n", rec,
+			 rmm_get_granule_state(rec));
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_rec_t *prec = (rmm_rec_t *)rec;
+	if (prec->state_running) {
+		rmm_info("destroy a running rec\n");
+		ret = RMI_ERROR_IN_USE;
+		goto out;
+	}
+
+	assert(prec->owner_rd->rec_count);
+	prec->owner_rd->rec_count --;
+
+	rmm_set_granule_state(rec, GS_DELEGATED);
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+
+static uint64_t enter_realm(rmm_rec_t *prec)
+{
+	uint64_t exit_code;
+	uint64_t saved_sre_el1 = read_icc_sre_el1();
+
+	/* We do not need to save any RMM system registers */
+	write_icc_sre_el1(ICC_SRE_EL1_SRE | ICC_SRE_EL1_DFB | ICC_SRE_EL1_DIB);
+
+	/* Restore Realm REC system registers */
+	write_elr_el2(prec->pc);
+	write_hcr_el2(prec->sysregs.hcr_el2);
+	write_spsr_el2(prec->sysregs.spsr_el2);
+	write_vtcr_el2(prec->sysregs.vtcr_el2);
+	write_vttbr_el2(prec->sysregs.vttbr_el2);
+	write_vmpidr_el2(prec->sysregs.vmpidr_el2);
+	el1_sysregs_context_restore(&(prec->sysregs.el1regs));
+
+	/* Restore Realm REC vgic cpu_if registers */
+	rmm_vgic_v3_restore_state(&prec->sysregs.cpu_if);
+
+	/* Save & Restore GP registers; ERET; exception jump back */
+	exit_code = __realm_enter(prec);
+
+	/* Save Realm system registers to rmm_rec_t.
+	 * No need to save all Realm EL2 registers because they aren't changed. */
+	el1_sysregs_context_save(&(prec->sysregs.el1regs));
+	prec->pc = read_elr_el2();
+	prec->sysregs.spsr_el2 = read_spsr_el2();
+	prec->sysregs.esr_el2 = read_esr_el2();
+	prec->sysregs.far_el2 = read_far_el2();
+	prec->sysregs.hpfar_el2 = read_hpfar_el2();
+
+	/* Save Realm REC vgic cpu_if registers */
+	rmm_vgic_v3_save_state(&prec->sysregs.cpu_if);
+
+	write_icc_sre_el1(saved_sre_el1);
+
+	return exit_code;
+}
+
+#define ESR_ISS_SRT(esr) (((esr) >> 16) & 0x1fUL)
+#define ESR_ISS_ISV_MASK (1UL << 24)
+#define ESR_ISS_WNR_MASK (1UL << 6)
+#define HPFAR_EL2_FIPA 0xFFFFFFFFFF0UL
+
+static void handle_data_abort_entry (rmm_rec_t *prec, rmi_rec_entry_t *pentry)
+{
+	if (!pentry->is_emulated_mmio)
+		return;
+#ifndef RME_DEBUG
+	assert(prec->emulatable_abort);
+#endif
+	prec->pc += 4;
+	if (!(prec->sysregs.esr_el2 & ESR_ISS_WNR_MASK))
+		prec->gprs[ESR_ISS_SRT(prec->sysregs.esr_el2)] = pentry->emulated_read_value;
+}
+
+static void handle_data_abort_exit (rmm_rec_t *prec, rmi_rec_exit_t *pexit)
+{
+	const rmm_realm_t *prd = prec->owner_rd;
+	pexit->reason = RMI_EXIT_SYNC;
+	pexit->hpfar = prec->sysregs.hpfar_el2;
+	uint64_t hpfar_el2_fipa = (prec->sysregs.hpfar_el2 & HPFAR_EL2_FIPA) << 8;
+	bool fault_in_protected = prd->par_base <= hpfar_el2_fipa &&
+		hpfar_el2_fipa < prd->par_base + prd->par_size;
+#ifdef RME_DEBUG
+	if (prec->sysregs.esr_el2 & ESR_ISS_ISV_MASK) {
+		if (fault_in_protected) {
+			static unsigned int warn_count = 0;
+			if (warn_count++ < 3) {
+				rmm_warn("treating protected data abort as emulatable."
+						" esr=0x%lx, hpfar=0x%lx, far=0x%lx, pc=0x%lx\n",
+						prec->sysregs.esr_el2, prec->sysregs.hpfar_el2,
+						prec->sysregs.far_el2, prec->pc);
+			}
+		}
+#else
+	if ((prec->sysregs.esr_el2 & ESR_ISS_ISV_MASK) && !fault_in_protected) {
+#endif
+		prec->emulatable_abort = true;
+		pexit->esr = prec->sysregs.esr_el2; // TODO mask away confidential bits
+		pexit->far_ = prec->sysregs.far_el2; // TODO mask away page offset
+		if (prec->sysregs.esr_el2 & ESR_ISS_WNR_MASK)
+			pexit->emulated_write_value = prec->gprs[ESR_ISS_SRT(prec->sysregs.esr_el2)];
+	} else { // non-emulatable
+		prec->emulatable_abort = false;
+		pexit->esr = prec->sysregs.esr_el2; // TODO mask away confidential bits
+	}
+}
+
+static inline uint64_t handle_exception_trap_enter(rmm_rec_t *prec, rmi_rec_entry_t *pentry)
+{
+	int i;
+
+	switch (EC_BITS(prec->sysregs.esr_el2)) {
+	case EC_WFE_WFI:
+		prec->pc += 4;
+		rmm_verbose("handle_rec_entry: wfe_wfi(0x%" PRIx64 ")\n", prec->pc);
+		break;
+	case EC_AARCH64_HVC:
+		for (i = 0; i < 7; i ++)
+			prec->gprs[i] = pentry->gprs[i];
+		break;
+	case EC_AARCH64_SMC:
+		if (prec->gprs[0] == PSCI_AFFINITY_INFO_AARCH64 ||
+				prec->gprs[0] == PSCI_CPU_ON_AARCH64) {
+			prec->gprs[0] = prec->psci_complete_result;
+		} else {
+			prec->gprs[0] = pentry->gprs[0];
+		}
+		prec->pc += 4;
+		break;
+	case EC_IABORT_LOWER_EL:
+		break;
+	case EC_DABORT_LOWER_EL:
+		handle_data_abort_entry(prec, pentry);
+		break;
+	default:
+		rmm_err("Unimplemented entry EC: esr=0x%lx\n", prec->sysregs.esr_el2);
+		return RMI_ERROR_REC;
+	}
+
+	return RMI_SUCCESS;
+}
+
+static uint64_t handle_rec_entry(rmm_rec_t *prec, rmi_rec_entry_t *pentry)
+{
+	int i;
+	uint64_t ret;
+
+	switch (prec->enter_reason) {
+	case REC_ENTER_REASON_FIRST_RUN:
+	case REC_ENTER_REASON_FIQ:
+	case REC_ENTER_REASON_IRQ:
+		break;
+	case REC_ENTER_REASON_TRAP:
+		ret = handle_exception_trap_enter(prec, pentry);
+		if (ret)
+			return ret;
+		break;
+	default:
+		rmm_err("Unknown enter reason: 0x%" PRIx64 "\n", prec->enter_reason);
+		return RMI_ERROR_REC;
+	}
+
+	prec->sysregs.cpu_if.vgic_hcr =
+		(pentry->gicv3_hcr & REC_ENTRY_ICH_HCR_VALID_MASK) | ICH_HCR_EN;
+	for (i = 0; i < 16; i++) {
+		prec->sysregs.cpu_if.vgic_lr[i] = pentry->gicv3_lrs[i];
+	}
+
+	return RMI_SUCCESS;
+}
+
+static void handle_psci_exit(rmm_rec_t *prec, rmi_rec_exit_t *pexit)
+{
+	pexit->reason = RMI_EXIT_PSCI;
+	pexit->gprs[0] = prec->gprs[0];
+	switch (prec->gprs[0]) {
+	case PSCI_AFFINITY_INFO_AARCH64:
+		pexit->gprs[0] = prec->gprs[0];
+		pexit->gprs[1] = prec->gprs[1];
+		pexit->gprs[2] = prec->gprs[2];
+		prec->psci_pending = true;
+		break;
+	case PSCI_CPU_OFF:
+		prec->runnable = false;
+		pexit->gprs[0] = prec->gprs[0];
+		break;
+	case PSCI_CPU_ON_AARCH64:
+		pexit->gprs[0] = prec->gprs[0];
+		pexit->gprs[1] = prec->gprs[1];
+		pexit->gprs[2] = prec->gprs[2];
+		pexit->gprs[3] = prec->gprs[3];
+		prec->psci_pending = true;
+		break;
+	case PSCI_CPU_SUSPEND_AARCH64:
+		pexit->gprs[0] = prec->gprs[0];
+		pexit->gprs[1] = prec->gprs[1];
+		pexit->gprs[2] = prec->gprs[2];
+		pexit->gprs[3] = prec->gprs[3];
+		break;
+	case PSCI_SYSTEM_OFF:
+		pexit->gprs[0] = prec->gprs[0];
+		prec->owner_rd->state = RS_SYSTEM_OFF;
+		break;
+	case PSCI_SYSTEM_RESET:
+		pexit->gprs[0] = prec->gprs[0];
+		prec->owner_rd->state = RS_SYSTEM_OFF;
+		break;
+	default:
+		rmm_err("handle_psci with unknown fid=0x%lx, pc=0x%lx\n", prec->gprs[0], prec->pc);
+	}
+}
+
+static void handle_exception_trap_exit(rmm_rec_t *prec, rmi_rec_exit_t *pexit)
+{
+	int i;
+	uint64_t err_code = EC_BITS(prec->sysregs.esr_el2);
+
+	switch (err_code) {
+	case EC_WFE_WFI:
+		pexit->reason = RMI_EXIT_SYNC;
+		pexit->esr = prec->sysregs.esr_el2;
+		rmm_verbose("handle_exception_trap: wfe_wfi(0x%" PRIx64 ")\n", prec->pc);
+		break;
+	case EC_AARCH64_HVC:
+		pexit->reason = RMI_EXIT_SYNC;
+		pexit->esr = prec->sysregs.esr_el2; // TODO mask only ESR_EL2.EC and ESR_EL2.ISS.imm16
+		for (i = 0; i < 7; i ++)
+			pexit->gprs[i] = prec->gprs[i];
+		break;
+	case EC_AARCH64_SMC:
+		handle_psci_exit(prec, pexit);
+		break;
+	case EC_IABORT_LOWER_EL:
+#ifdef RME_DEBUG
+		{
+			static unsigned int warn_count = 0;
+			if (warn_count++ < 3) {
+				rmm_warn("Treating ins abort as data abort."
+						" esr=0x%lx, hpfar=0x%lx, far=0x%lx, pc=0x%lx\n",
+						prec->sysregs.esr_el2, prec->sysregs.hpfar_el2,
+						prec->sysregs.far_el2, prec->pc);
+			}
+		}
+		// Intentional fallthrough
+#else
+		pexit->reason = RMI_EXIT_SYNC;
+		pexit->esr = prec->sysregs.esr_el2; // TODO mask away confidential bits
+		pexit->hpfar = prec->sysregs.hpfar_el2;
+		break;
+#endif
+	case EC_DABORT_LOWER_EL:
+		handle_data_abort_exit(prec, pexit);
+		break;
+	default:
+		rmm_err("Unimplemented exit EC: esr=0x%lx, pc=0x%lx, x0=0x%lx\n", prec->sysregs.esr_el2, prec->pc, prec->gprs[0]);
+		__builtin_unreachable();
+	}
+
+	return;
+}
+
+static uint64_t handle_rec_exit(rmm_rec_t *prec, rmi_rec_exit_t *pexit, uint64_t exit_code)
+{
+	int i;
+
+	pexit->cntv_ctl =  read_ctx_reg(&(prec->sysregs.el1regs), CTX_CNTV_CTL_EL0);
+	pexit->cntv_cval = read_ctx_reg(&(prec->sysregs.el1regs), CTX_CNTV_CVAL_EL0);
+	/* disable el0 vtimer */
+	write_cntv_ctl_el0(0);
+
+	pexit->cntp_ctl =  read_ctx_reg(&(prec->sysregs.el1regs), CTX_CNTP_CTL_EL0);
+	pexit->cntp_cval = read_ctx_reg(&(prec->sysregs.el1regs), CTX_CNTP_CVAL_EL0);
+	/* disable el0 ptimer */
+	write_cntp_ctl_el0(0);
+
+	/* Expose non-confidential gicv3 context info of rec to NS through rec_exit */
+	pexit->gicv3_hcr = prec->sysregs.cpu_if.vgic_hcr & REC_EXIT_ICH_HCR_VALID_MASK;
+	pexit->gicv3_vmcr = prec->sysregs.cpu_if.vgic_vmcr;
+	/* ICH_MISR_EL2 is RO, so we do not need to store it in rec cpu_if */
+	pexit->gicv3_misr = read_ich_misr_el2();
+	for (i = 0; i < 16; i++)
+		pexit->gicv3_lrs[i] = prec->sysregs.cpu_if.vgic_lr[i];
+
+	rmm_verbose("handle_rec_exit: exit_code(0x%" PRIx64 ") cntv(0x%"
+			PRIx64 ", 0x%" PRIx64 ") cntp(0x%" PRIx64 ", 0x%" PRIx64
+			") gicv3_hcr(0x%" PRIx64 ") gicv3_vmcr(0x%" PRIx64
+			") gicv3_misr(0x%" PRIx64 ") gicv3_lrs(0x%" PRIx64 " 0x%" PRIx64
+			" 0x%" PRIx64 " 0x%" PRIx64 ")\n",
+			exit_code, pexit->cntv_ctl, pexit->cntv_cval,
+			pexit->cntp_ctl, pexit->cntp_cval,
+			pexit->gicv3_hcr, pexit->gicv3_vmcr, pexit->gicv3_misr,
+			pexit->gicv3_lrs[0], pexit->gicv3_lrs[1], pexit->gicv3_lrs[2], pexit->gicv3_lrs[3]);
+
+	switch (exit_code) {
+	case ARM_EXCEPTION_FIQ:
+		pexit->reason = RMI_EXIT_FIQ;
+		pexit->esr = 0;
+		prec->enter_reason = REC_ENTER_REASON_FIQ;
+		break;
+	case ARM_EXCEPTION_IRQ:
+		pexit->reason = RMI_EXIT_IRQ;
+		pexit->esr = 0;
+		prec->enter_reason = REC_ENTER_REASON_IRQ;
+		break;
+	case ARM_EXCEPTION_TRAP:
+		handle_exception_trap_exit(prec, pexit);
+		prec->enter_reason = REC_ENTER_REASON_TRAP;
+		break;
+	default:
+		rmm_err("Unhandled exception exit: %d",(int)exit_code);
+		return RMI_ERROR_INTERNAL;
+	}
+
+	return RMI_SUCCESS;
+}
+
+/* Return true if need to call enter_realm immidiately.
+ * Return false if need to return to host. */
+static bool handle_internal_rec_exit (rmm_rec_t *prec, uint64_t exit_code)
+{
+	if (exit_code == ARM_EXCEPTION_TRAP) {
+		if (EC_BITS(prec->sysregs.esr_el2) == EC_AARCH64_SMC) {
+			uint64_t retval;
+			rmm_realm_t *prd;
+			switch (prec->gprs[0]) {
+			case PSCI_CPU_OFF:
+			case PSCI_CPU_SUSPEND_AARCH64:
+			case PSCI_SYSTEM_OFF:
+			case PSCI_SYSTEM_RESET:
+				return false;
+			case PSCI_VERSION:
+				retval = trp_smc(set_smc_args(PSCI_VERSION, 0, 0, 0, 0, 0, 0, 0));
+				prec->gprs[0] = retval;
+				prec->pc += 4;
+				return true;
+			case PSCI_FEATURES:
+				retval = trp_smc(set_smc_args(PSCI_FEATURES, prec->gprs[1], 0, 0, 0, 0, 0, 0));
+				prec->gprs[0] = (uint64_t)retval;
+				prec->pc += 4;
+				return true;
+			case PSCI_CPU_ON_AARCH64:
+				prd = prec->owner_rd;
+				if (mpidr2recindex(prec->gprs[1]) >= prd->rec_index) {
+					prec->gprs[0] = PSCI_E_INVALID_PARAMS;
+					prec->pc += 4;
+					return true;
+				}
+				if (prec->gprs[2] < prd->par_base ||
+						prec->gprs[2] >= prd->par_base + prd->par_size) {
+					prec->gprs[0] = PSCI_E_INVALID_ADDRESS;
+					prec->pc += 4;
+					return true;
+				}
+				return false;
+			case PSCI_AFFINITY_INFO_AARCH64:
+				prd = prec->owner_rd;
+				if (mpidr2recindex(prec->gprs[1]) >= prd->rec_index) {
+					prec->gprs[0] = PSCI_E_INVALID_PARAMS;
+					prec->pc += 4;
+					return true;
+				}
+				if (prec->gprs[2] != 0) {
+					prec->gprs[0] = PSCI_E_INVALID_PARAMS;
+					prec->pc += 4;
+					return true;
+				}
+				return false;
+			default:
+				prec->gprs[0] = PSCI_E_NOT_SUPPORTED;
+				prec->pc += 4;
+				return true;
+			}
+		}
+	}
+	return false;
+}
+
+static trp_args_t *trp_rec_enter (uint64_t rec, uint64_t run_ptr)
+{
+	uint64_t ret;
+	uint64_t exit_code;
+	spin_lock(&rmm_lock);
+	rmm_verbose("enter REC 0x%" PRIx64 " 0x%" PRIx64 "\n", rec, run_ptr);
+
+	if (!rmm_assert_granule_state(rec, GS_REC)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(run_ptr, GS_UNDELEGATED)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	rmm_rec_t *prec = (rmm_rec_t *)rec;
+	if (prec->state_running) {
+		ret = RMI_ERROR_IN_USE;
+		goto out;
+	}
+	if (!(prec->runnable)) {
+		rmm_info("rec enter but not runnable.\n");
+		ret = RMI_ERROR_REC;
+		goto out;
+	}
+	if (prec->psci_pending) {
+		rmm_info("rec enter but psci_pending\n");
+		ret = RMI_ERROR_REC;
+		goto out;
+	}
+	rmm_realm_state_e realm_state = prec->owner_rd->state;
+	if (realm_state == RS_NEW || realm_state == RS_SYSTEM_OFF) {
+		rmm_info("rec enter with rd.state=%d\n", realm_state);
+		ret = RMI_ERROR_REALM_STATE;
+		goto out;
+	}
+
+	rmi_rec_entry_t rec_entry;
+	if (!rmm_read_ns(&rec_entry, (void *)run_ptr, sizeof(rec_entry))) {
+		ret = RMI_ERROR_MEMORY;
+		goto out;
+	}
+	if (rec_entry.is_emulated_mmio && !prec->emulatable_abort) {
+#if RME_DEBUG
+		static unsigned int warn_count = 0;
+		if (warn_count++ < 3) {
+			rmm_warn("rec enter: is_emulated_mmio but not emulatable."
+					" esr=0x%lx, hpfar=0x%lx, far=0x%lx, pc=0x%lx\n",
+					prec->sysregs.esr_el2, prec->sysregs.hpfar_el2,
+					prec->sysregs.far_el2, prec->pc);
+		}
+#else
+		rmm_info("rec enter: is_emulated_mmio but not emulatable\n");
+		ret = RMI_ERROR_REC;
+		goto out;
+#endif
+	}
+	/*
+	if (rec_entry.gicv3_hcr & (~REC_ENTRY_ICH_HCR_VALID_MASK)) {
+		rmm_info("rec enter: invalid gicv3_hcr 0x%lx\n", rec_entry.gicv3_hcr);
+		ret = RMI_ERROR_REC;
+		goto out;
+	}
+	*/
+
+	ret = handle_rec_entry(prec, &rec_entry);
+	if (ret != RMI_SUCCESS)
+		goto out;
+
+	/* Disable the EL2 physical Timer used by host(kvm) */
+	uint64_t cnthp_ctl_el2 = read_cnthp_ctl_el2();
+	rmm_verbose("enter_realm enter(pc: 0x%" PRIx64 ") cnthp_ctl_el2:(0x%" PRIx64 ")\n",
+		    prec->pc, cnthp_ctl_el2);
+	write_cnthp_ctl_el2(cnthp_ctl_el2 & ~CNTHP_CTL_ENABLE_BIT);
+	isb();
+
+	prec->state_running = true;
+
+	spin_unlock(&rmm_lock);
+	do {
+		exit_code = enter_realm(prec);
+	} while (handle_internal_rec_exit(prec, exit_code));
+	spin_lock(&rmm_lock);
+
+	prec->state_running = false;
+	/* Enable the EL2 physical Timer */
+	/*
+	if (cnthp_ctl_el2 & CNTHP_CTL_IT_STAT) {
+		cnthp_ctl_el2 &= ~CNTHP_CTL_IT_MASK;
+		write_cnthp_ctl_el2(cnthp_ctl_el2);
+	}*/
+
+	write_cnthp_ctl_el2(cnthp_ctl_el2);
+	isb();
+
+	rmi_rec_exit_t rec_exit;
+	memset(&rec_exit, 0, sizeof(rec_exit));
+	ret = handle_rec_exit(prec, &rec_exit, exit_code);
+	if (ret != RMI_SUCCESS)
+		goto out;
+
+	if (!rmm_write_ns((void *)(run_ptr + sizeof(rec_entry)), &rec_exit, sizeof(rec_exit))) {
+		ret = RMI_ERROR_MEMORY; // TODO: realm executed but return error seems wrong.
+		goto out;
+	}
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_create_rtt (unsigned long long rtt, unsigned long long rd, unsigned long long map_addr, int level)
+{
+	unsigned long long ret;
+
+	spin_lock(&rmm_lock);
+	rmm_verbose("create rtt 0x%llx 0x%llx 0x%llx 0x%d\n", rtt, rd, map_addr, level);
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(rtt, GS_DELEGATED)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level - 1)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	const rmm_realm_t *prd = (const rmm_realm_t *)rd;
+	if (map_addr >> prd->ipa_width) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (level <= prd->rtt_level_start || level > RMM_RTT_PAGE_LEVEL) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, prd, map_addr, level - 1);
+
+	if (walk_result.level != level - 1) {
+		ret = RMI_ERROR_RTT_WALK;
+		goto out;
+	}
+	rmm_rtt_entry_state_e state = rmm_get_rtte_state(&walk_result);
+	if (state == ES_ASSIGNED || state == ES_VALID || state == ES_VALID_NS) {
+		if (level != RMM_RTT_PAGE_LEVEL) {
+			ret = RMI_ERROR_RTT_ENTRY;
+			goto out;
+		}
+		uint64_t parent_rtte = *(walk_result.rtte);
+		for (int i = 511; i >= 0; i --)
+			((uint64_t *)rtt)[i] = parent_rtte + (i << 12UL);
+	} else if (state == ES_UNASSIGNED) {
+		memset((void *)rtt, 0, RMM_GRANULE_SIZE);
+	} else if (state == ES_DESTROYED) {
+		for (int i = 511; i >= 0; i --)
+			((uint64_t *)rtt)[i] = 1UL << RTT_DESTROYED_BIT;
+	} else if (state == ES_TABLE) {
+		ret = RMI_ERROR_RTT_ENTRY;
+		goto out;
+	} else {
+		assert(false);
+	}
+
+	rmm_set_granule_state(rtt, GS_RTT);
+	rmm_set_rtte_table(&walk_result, rtt);
+	ret = 0;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_destroy_rtt (unsigned long long rtt, unsigned long long rd, unsigned long long map_addr, int level)
+{
+	unsigned long long ret;
+
+	spin_lock(&rmm_lock);
+	rmm_verbose("destroy rtt 0x%llx 0x%llx 0x%llx 0x%d\n", rtt, rd, map_addr, level);
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(rtt, GS_RTT)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level - 1)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	const rmm_realm_t *rdp = (const rmm_realm_t *)rd;
+	if (map_addr >> rdp->ipa_width) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (level <= rdp->rtt_level_start || level > RMM_RTT_PAGE_LEVEL) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, rdp, map_addr, level - 1);
+
+	if (walk_result.level != level - 1) {
+		ret = RMI_ERROR_RTT_WALK;
+		goto out;
+	}
+	if (rmm_get_rtte_state(&walk_result) != ES_TABLE) {
+		ret = RMI_ERROR_RTT_ENTRY;
+		goto out;
+	}
+	if (rmm_get_rtte_outaddr(&walk_result) != rtt) {
+		ret = RMI_ERROR_RTT_ENTRY;
+		goto out;
+	}
+
+	if (!rmm_rtt_fold(&walk_result)) {
+		ret = RMI_ERROR_IN_USE;
+		goto out;
+	}
+	rmm_set_granule_state(rtt, GS_DELEGATED);
+	ret = 0;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static int trp_data_create_internal (uint64_t data, uint64_t rd, uint64_t map_addr, bool has_src, uint64_t src, int level)
+{
+	if (level != RMM_RTT_BLOCK_LEVEL && level != RMM_RTT_PAGE_LEVEL)
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state_level(data, level, GS_DELEGATED))
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state(rd, GS_RD))
+		return RMI_ERROR_INPUT;
+	if (has_src && !rmm_assert_granule_state_level(src, level, GS_UNDELEGATED))
+		return RMI_ERROR_INPUT;
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level))
+		return RMI_ERROR_INPUT;
+	const rmm_realm_t *rdp = (const rmm_realm_t *)rd;
+
+	if (has_src && rdp->state != RS_NEW) {
+#if RME_DEBUG
+		static unsigned int warn_count = 3;
+		if (warn_count++ < 3) {
+			rmm_warn("late data_create. state=%d, data=0x%lx, map_addr=0x%lx, src=0x%lx\n",
+					rdp->state, data, map_addr, src);
+		}
+#else
+		return RMI_ERROR_REALM_STATE;
+#endif
+	}
+
+	const uint64_t data_size = RMM_GRANULE_SIZE << ((3 - level) * 9);
+	if (map_addr < rdp->par_base ||
+			map_addr + data_size > rdp->par_base + rdp->par_size)
+		return RMI_ERROR_INPUT;
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, rdp, map_addr, level);
+
+	if (walk_result.level != level)
+		return RMI_ERROR_RTT_WALK;
+	if (rmm_get_rtte_state(&walk_result) != ES_UNASSIGNED)
+		return RMI_ERROR_RTT_ENTRY;
+
+	if (has_src) {
+		/* rmm_read_ns doesn't support multiple page, so we have to break up here. */
+		for (uint64_t offset = 0; offset < data_size; offset += RMM_GRANULE_SIZE) {
+			if (!rmm_read_ns((void *)(data + offset), (void *)(src + offset), RMM_GRANULE_SIZE))
+				return RMI_ERROR_MEMORY;
+		}
+	} else {
+		memset((void *)data, 0, data_size);
+	}
+	clean_dcache_range(data, data_size);
+	for (uint64_t offset = 0; offset < data_size; offset += RMM_GRANULE_SIZE) {
+		rmm_set_granule_state(data + offset, GS_DATA);
+	}
+	rmm_set_rtte(&walk_result, data, false);
+	return 0;
+}
+
+static trp_args_t *trp_data_create (unsigned long long data, unsigned long long rd, unsigned long long map_addr, unsigned long long src, unsigned long long level)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("create data 0x%llx 0x%llx 0x%llx 0x%llx 0x%llx\n", data, rd, map_addr, src, level);
+	int ret = trp_data_create_internal(data, rd, map_addr, true, src, level);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_data_create_unknown (unsigned long long data, unsigned long long rd, unsigned long long map_addr, unsigned long long level)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("create data unknown 0x%llx 0x%llx 0x%llx 0x%llx\n", data, rd, map_addr, level);
+	int ret = trp_data_create_internal(data, rd, map_addr, false, 0, level);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static int trp_data_destroy_internal (uint64_t rd, uint64_t map_addr, int level)
+{
+	if (level != RMM_RTT_BLOCK_LEVEL && level != RMM_RTT_PAGE_LEVEL)
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state(rd, GS_RD))
+		return RMI_ERROR_INPUT;
+	if (!rmm_is_addr_granule_aligned(map_addr))
+		return RMI_ERROR_INPUT;
+	const rmm_realm_t *rdp = (const rmm_realm_t *)rd;
+	const uint64_t data_size = RMM_GRANULE_SIZE << ((3 - level) * 9);
+	if (map_addr < rdp->par_base || map_addr + data_size > rdp->par_base + rdp->par_size)
+		return RMI_ERROR_INPUT;
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, rdp, map_addr, level);
+
+	if (walk_result.level != level)
+		return RMI_ERROR_RTT_WALK;
+	if (rmm_get_rtte_state(&walk_result) != ES_ASSIGNED)
+		return RMI_ERROR_RTT_ENTRY;
+	if (!rmm_assert_granule_state_level(rmm_get_rtte_outaddr(&walk_result), level, GS_DATA)) {
+		rmm_err("Destroying data at %lx but found type %d\n",
+				(unsigned long)rmm_get_rtte_outaddr(&walk_result),
+				rmm_get_granule_state(rmm_get_rtte_outaddr(&walk_result)));
+	}
+
+	for (uint64_t offset = 0; offset < data_size; offset += RMM_GRANULE_SIZE) {
+		rmm_set_granule_state(rmm_get_rtte_outaddr(&walk_result) + offset, GS_DELEGATED);
+	}
+	rmm_set_rtte_destroyed(&walk_result);
+	return 0;
+}
+
+static trp_args_t *trp_data_destroy (unsigned long long rd, unsigned long long map_addr, unsigned long long level)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("destroy data 0x%llx 0x%llx 0x%llx\n", rd, map_addr, level);
+	int ret = trp_data_destroy_internal(rd, map_addr, level);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static int trp_data_dispose_internal (uint64_t rd, uint64_t rec, uint64_t map_addr, int level)
+{
+	if (level != RMM_RTT_BLOCK_LEVEL && level != RMM_RTT_PAGE_LEVEL)
+		return RMI_ERROR_INPUT;
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level))
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state(rd, GS_RD))
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state(rec, GS_REC))
+		return RMI_ERROR_INPUT;
+	const rmm_realm_t *prd = (const rmm_realm_t *)rd;
+	const rmm_rec_t *prec = (const rmm_rec_t *)rec;
+	if (prec->state_running)
+		return RMI_ERROR_IN_USE;
+	if (prec->owner_rd != prd)
+		return RMI_ERROR_OWNER;
+	//TODO: prec->dispose_base and dispose_size
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, prd, map_addr, level);
+
+	if (walk_result.level != level)
+		return RMI_ERROR_RTT_WALK;
+	if (rmm_get_rtte_state(&walk_result) != ES_DESTROYED)
+		return RMI_ERROR_RTT_ENTRY;
+
+	rmm_set_rtte(&walk_result, 0, false);
+	return 0;
+}
+
+static trp_args_t *trp_data_dispose (unsigned long long rd, unsigned long long rec, unsigned long long map_addr, int level)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("dispose data 0x%llx 0x%llx0x%llx %d\n", rd, rec, map_addr, level);
+	int ret = trp_data_dispose_internal(rd, rec, map_addr, level);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static int trp_map_unmap_internal(unsigned long long rd, unsigned long long map_addr, int level, int is_map, unsigned long long ns_rtte)
+{
+	if (level != RMM_RTT_BLOCK_LEVEL && level != RMM_RTT_PAGE_LEVEL)
+		return RMI_ERROR_INPUT;
+	if (!rmm_assert_granule_state(rd, GS_RD))
+		return RMI_ERROR_INPUT;
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level))
+		return RMI_ERROR_INPUT;
+	if (is_map && ns_rtte) {
+		uint64_t oa = ns_rtte & RTT_OA_MASK; // should we check if it's ns?
+		if (!rmm_is_addr_delegable(oa)) // TODO: is this same as PaIsValid?
+			return RMI_ERROR_INPUT;
+		if (!rmm_is_addr_rtt_level_aligned(oa, level))
+			return RMI_ERROR_INPUT;
+		//TODO check RttEntryIsValidForUnprotected
+	}
+	const rmm_realm_t *prd = (const rmm_realm_t *)rd;
+	if (map_addr >> prd->ipa_width)
+		return RMI_ERROR_INPUT;
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, prd, map_addr, level);
+
+	if (walk_result.level != level)
+		return RMI_ERROR_RTT_WALK;
+	rmm_rtt_entry_state_e state = rmm_get_rtte_state(&walk_result);
+	if (is_map) {
+		if (ns_rtte) {
+			if (state != ES_UNASSIGNED && state != ES_DESTROYED)
+				return RMI_ERROR_RTT_ENTRY;
+			rmm_set_rtte_ns(&walk_result, ns_rtte);
+		} else {
+			if (state != ES_ASSIGNED)
+				return RMI_ERROR_RTT_ENTRY;
+			rmm_set_rtte_valid(&walk_result, true);
+		}
+	} else {
+		if (ns_rtte) {
+			if (state != ES_VALID_NS)
+				return RMI_ERROR_RTT_ENTRY;
+			if (rmm_get_rtte_outaddr(&walk_result) != ns_rtte)
+				return RMI_ERROR_RTT_ENTRY;
+			rmm_set_rtte(&walk_result, 0, false);
+		} else {
+			if (state != ES_VALID)
+				return RMI_ERROR_RTT_ENTRY;
+			rmm_set_rtte_valid(&walk_result, false);
+		}
+	}
+
+	/* TLB maintenance
+	 * TODO: check VMID switch and barrier etc*/
+	asm volatile("tlbi ipas2le1is, %0" : : "r" (TLBI_ADDR(map_addr)));
+	return 0;
+}
+
+static trp_args_t *trp_map_protected (unsigned long long rd, unsigned long long map_addr, int level)
+{
+	spin_lock(&rmm_lock);
+	unsigned long long ret = trp_map_unmap_internal(rd, map_addr, level, true, 0);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_map_unprotected (unsigned long long rd, unsigned long long map_addr, int level, unsigned long long rtte)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("map unprotected 0x%llx 0x%llx 0x%d 0x%llx\n", rd, map_addr, level, rtte);
+	unsigned long long ret = trp_map_unmap_internal(rd, map_addr, level, true, rtte);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_unmap_protected (unsigned long long rd, unsigned long long map_addr, int level)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("unmap protected 0x%llx 0x%llx 0x%d\n", rd, map_addr, level);
+	unsigned long long ret = trp_map_unmap_internal(rd, map_addr, level, false, 0);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_unmap_unprotected (unsigned long long rd, unsigned long long map_addr, int level, unsigned long long rtte)
+{
+	spin_lock(&rmm_lock);
+	rmm_verbose("unmap unprotected 0x%llx 0x%llx 0x%d 0x%llx\n", rd, map_addr, level, rtte);
+	unsigned long long ret = trp_map_unmap_internal(rd, map_addr, level, false, rtte);
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_read_rtt_entry (unsigned long long rd, unsigned long long map_addr, int level)
+{
+	unsigned long long ret;
+	spin_lock(&rmm_lock);
+	rmm_verbose("read_rtt_entry 0x%llx 0x%llx 0x%d\n", rd, map_addr, level);
+	if (!rmm_assert_granule_state(rd, GS_RD)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_is_addr_rtt_level_aligned(map_addr, level)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	const rmm_realm_t *prd = (const rmm_realm_t *)rd;
+	if (map_addr >> prd->ipa_width) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (level < prd->rtt_level_start || level > RMM_RTT_PAGE_LEVEL) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	rmm_rtt_walk_result_t walk_result;
+	rmm_rtt_walk(&walk_result, prd, map_addr, level);
+
+	if (walk_result.level != level) {
+		ret = RMI_ERROR_RTT_WALK;
+		goto out;
+	}
+
+	uint64_t rtte = *(walk_result.rtte);
+	rmm_verbose("read entry: addr=%lx, level=%d, rtte=%lx\n", (unsigned long)map_addr, level, (unsigned long)rtte);
+	rmm_rtt_entry_state_e state = rmm_get_rtte_state(&walk_result);
+	switch (state) {
+	case ES_UNASSIGNED:
+	case ES_DESTROYED:
+		rtte = 0;
+		break;
+	case ES_ASSIGNED:
+	case ES_VALID:
+	case ES_TABLE:
+		rtte &= RTT_OA_MASK;
+		break;
+	case ES_VALID_NS:
+		rtte &= RTT_OA_MASK | 0x3fcUL; // 0x3fc = MemAttr[2:5], S2AP[6:7], SH[8:9]
+		break;
+	default:
+		assert(false);
+	}
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	if (ret == RMI_SUCCESS)
+		return set_smc_args(RMMD_RMI_REQ_COMPLETE, 0, level, state, rtte, 0, 0, 0);
+	else
+		return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+static trp_args_t *trp_psci_complete (uint64_t calling_rec, uint64_t target_rec)
+{
+	unsigned long long ret;
+	spin_lock(&rmm_lock);
+	rmm_verbose("trp_psci_complete 0x%lx 0x%lx\n", calling_rec, target_rec);
+	if (calling_rec == target_rec) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(calling_rec, GS_REC)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (!rmm_assert_granule_state(target_rec, GS_REC)) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	rmm_rec_t *prec_calling = (rmm_rec_t *)calling_rec;
+	rmm_rec_t *prec_target = (rmm_rec_t *)target_rec;
+	if (!prec_calling->psci_pending) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (prec_calling->owner_rd != prec_target->owner_rd) {
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+	if (prec_calling->gprs[0] != PSCI_AFFINITY_INFO_AARCH64 && prec_calling->gprs[0] != PSCI_CPU_ON_AARCH64) {
+		rmm_err("trp_psci_complete unexpected x0=0x%lx\n", prec_calling->gprs[0]);
+		ret = RMI_ERROR_INPUT;
+		goto out;
+	}
+
+	prec_calling->psci_pending = false;
+	if (prec_calling->gprs[0] == PSCI_AFFINITY_INFO_AARCH64) {
+		prec_calling->psci_complete_result = prec_target->runnable ? AFF_STATE_ON : AFF_STATE_OFF;
+	} else {
+		assert(prec_calling->gprs[0] == PSCI_CPU_ON_AARCH64);
+		if (prec_target->runnable) {
+			prec_calling->psci_complete_result = PSCI_E_ALREADY_ON;
+		} else {
+			prec_calling->psci_complete_result = PSCI_E_SUCCESS;
+			prec_target->gprs[0] = prec_calling->gprs[3];
+			for (int i = 1; i <= 31; i ++)
+				prec_target->gprs[i] = 0;
+			prec_target->runnable = true;
+			prec_target->pc = prec_calling->gprs[2];
+		}
+	}
+	prec_calling->gprs[1] = 0;
+	prec_calling->gprs[2] = 0;
+	prec_calling->gprs[3] = 0;
+	ret = RMI_SUCCESS;
+
+out:
+	spin_unlock(&rmm_lock);
+	return set_smc_args(RMMD_RMI_REQ_COMPLETE, ret, 0, 0, 0, 0, 0, 0);
+}
+
+
 /*******************************************************************************
  * Main RMI SMC handler function
  ******************************************************************************/
-trp_args_t *trp_rmi_handler(unsigned long fid, unsigned long long x1)
+trp_args_t *trp_rmi_handler(unsigned long fid, unsigned long long x1,
+			    unsigned long long x2, unsigned long long x3,
+				unsigned long long x4, unsigned long long x5)
 {
 	switch (fid) {
 	case RMI_RMM_REQ_VERSION:
@@ -128,8 +1542,52 @@ trp_args_t *trp_rmi_handler(unsigned long fid, unsigned long long x1)
 		return trp_asc_mark_realm(x1);
 	case RMI_RMM_GRANULE_UNDELEGATE:
 		return trp_asc_mark_nonsecure(x1);
+	case RMI_RMM_DATA_CREATE:
+		return trp_data_create(x1, x2, x3, x4, 3);
+	case RMI_RMM_DATA_CREATE_UNKNOWN:
+		return trp_data_create_unknown(x1, x2, x3, 3);
+	case RMI_RMM_DATA_CREATE_LEVEL:
+		return trp_data_create(x1, x2, x3, x4, x5);
+	case RMI_RMM_DATA_CREATE_UNKNOWN_LEVEL:
+		return trp_data_create_unknown(x1, x2, x3, x4);
+	case RMI_RMM_DATA_DESTROY:
+		return trp_data_destroy(x1, x2, 3);
+	case RMI_RMM_DATA_DESTROY_LEVEL:
+		return trp_data_destroy(x1, x2, x3);
+	case RMI_RMM_DATA_DISPOSE:
+		return trp_data_dispose(x1, x2, x3, x4);
+	case RMI_RMM_REALM_ACTIVATE:
+		return trp_activate_realm(x1);
+	case RMI_RMM_REALM_CREATE:
+		return trp_create_realm(x1, x2);
+	case RMI_RMM_REALM_DESTROY:
+		return trp_destroy_realm(x1);
+	case RMI_RMM_REC_CREATE:
+		return trp_rec_create(x1, x2, x3, x4);
+	case RMI_RMM_REC_DESTROY:
+		return trp_rec_destroy(x1);
+	case RMI_RMM_REC_ENTER:
+		return trp_rec_enter(x1, x2);
+	case RMI_RMM_RTT_CREATE:
+		return trp_create_rtt(x1, x2, x3, x4);
+	case RMI_RMM_RTT_DESTROY:
+		return trp_destroy_rtt(x1, x2, x3, x4);
+	case RMI_RMM_RTT_MAP_UNPROTECTED:
+		return trp_map_unprotected(x1, x2, x3, x4);
+	case RMI_RMM_RTT_MAP_PROTECTED:
+		return trp_map_protected(x1, x2, x3);
+	case RMI_RMM_RTT_READ_ENTRY:
+		return trp_read_rtt_entry(x1, x2, x3);
+	case RMI_RMM_RTT_UNMAP_UNPROTECTED:
+		return trp_unmap_unprotected(x1, x2, x3, x4);
+	case RMI_RMM_RTT_UNMAP_PROTECTED:
+		return trp_unmap_protected(x1, x2, x3);
+	case RMI_RMM_PSCI_COMPLETE:
+		return trp_psci_complete(x1, x2);
+	case RMI_RMM_FEATURES:
+		return set_smc_args(RMMD_RMI_REQ_COMPLETE, RMI_ERROR_NOT_SUPPORTED, 0, 0, 0, 0, 0, 0);
 	default:
-		ERROR("Invalid SMC code to %s, FID %lu\n", __func__, fid);
+		rmm_err("Invalid SMC code to %s, FID %lu\n", __func__, fid);
 	}
 	return set_smc_args(SMC_UNK, 0, 0, 0, 0, 0, 0, 0);
 }
diff --git a/services/std_svc/rmmd/trp/trp_private.h b/services/std_svc/rmmd/trp/trp_private.h
index 43a4a4bfd..47482ef75 100644
--- a/services/std_svc/rmmd/trp/trp_private.h
+++ b/services/std_svc/rmmd/trp/trp_private.h
@@ -7,6 +7,11 @@
 #ifndef TRP_PRIVATE_H
 #define TRP_PRIVATE_H
 
+#define ARM_EXCEPTION_IRQ   0
+#define ARM_EXCEPTION_FIQ   1
+#define ARM_EXCEPTION_EL1_SERROR    2
+#define ARM_EXCEPTION_TRAP  3
+
 /* Definitions to help the assembler access the SMC/ERET args structure */
 #define TRP_ARGS_SIZE		TRP_ARGS_END
 #define TRP_ARG0		0x0
@@ -22,6 +27,37 @@
 #ifndef __ASSEMBLER__
 
 #include <stdint.h>
+#include <assert.h>
+#include <lib/xlat_tables/xlat_tables_v2.h>
+#include <lib/el3_runtime/aarch64/context.h>
+#include "vgic-v3.h"
+
+#define rmm_err(fmt, ...) \
+	ERROR("rmm : " fmt, ## __VA_ARGS__)
+
+#define rmm_info(fmt, ...) \
+	INFO("rmm : " fmt, ## __VA_ARGS__)
+
+#define rmm_notice(fmt, ...) \
+	NOTICE("rmm : " fmt, ## __VA_ARGS__)
+
+#define rmm_warn(fmt, ...) \
+	WARN("rmm : " fmt, ## __VA_ARGS__)
+
+#define rmm_verbose(fmt, ...) \
+	VERBOSE("rmm : " fmt, ## __VA_ARGS__)
+
+
+// TODO reference them from platform. using 4GB for now
+#define RMM_NUM_GRANULES2 20
+#define RMM_NUM_GRANULES (1UL << RMM_NUM_GRANULES2)
+#define RMM_GRANULE_SIZE2 12
+#define RMM_GRANULE_SIZE (1UL << RMM_GRANULE_SIZE2)
+
+#define RMM_RTT_BLOCK_LEVEL 2
+#define RMM_RTT_PAGE_LEVEL 3
+
+typedef uint64_t rmm_addr_t;
 
 /* Data structure to hold SMC arguments */
 typedef struct trp_args {
@@ -31,18 +67,146 @@ typedef struct trp_args {
 #define write_trp_arg(args, offset, val) (((args)->regs[offset >> 3])	\
 					 = val)
 
+typedef enum rmm_granule_state {
+	GS_UNDELEGATED, GS_DELEGATED, GS_DATA, GS_RD, GS_REC, GS_REC_AUX, GS_RTT
+} rmm_granule_state_e;
+
+typedef enum rmm_physical_address_space {
+	PAS_NS, PAS_REALM, PAS_ROOT, PAS_SECURE
+} rmm_physical_address_space_e;
+
+typedef enum rmm_realm_state {
+	RS_ACTIVE, RS_NEW, RS_SYSTEM_OFF
+} rmm_realm_state_e;
+
+typedef enum rmm_rtt_entry_state {
+	ES_ASSIGNED, ES_DESTROYED, ES_TABLE, ES_UNASSIGNED, ES_VALID, ES_VALID_NS
+} rmm_rtt_entry_state_e;
+
+typedef struct rmm_system_registers {
+	el1_sysregs_t el1regs;
+	uint64_t hcr_el2; // in
+	uint64_t spsr_el2; // in & out
+	uint64_t vtcr_el2; // in
+	uint64_t vttbr_el2; // in
+	uint64_t vmpidr_el2; // in
+	uint64_t esr_el2; // out
+	uint64_t far_el2; // out
+	uint64_t hpfar_el2; // out
+	vgic_v3_cpu_if_t cpu_if;
+} rmm_system_registers_t;
+
+typedef struct rmm_realm {
+	//TODO measurement;
+	//TODO measurement_algo;
+	uint64_t par_base;
+	uint64_t par_size;
+	uint64_t rec_index;
+	uint64_t rec_count; // number of RECs currently owned. Checked when destroying realm
+	uint64_t rtt_base;
+	uint64_t rtt_num_start;
+	unsigned int ipa_width;
+	unsigned int vmid;
+	int rtt_level_start;
+	rmm_realm_state_e state;
+} rmm_realm_t;
+
+#define REC_ENTER_REASON_FIRST_RUN	0
+#define REC_ENTER_REASON_IRQ		1
+#define REC_ENTER_REASON_FIQ		2
+#define REC_ENTER_REASON_EL1_SERROR	3
+#define REC_ENTER_REASON_TRAP		4
+
+typedef struct rmm_rec {
+	uint64_t gprs[32];
+	rmm_system_registers_t sysregs;
+	rmm_realm_t *owner_rd;
+	uint64_t dispose_base;
+	uint64_t dispose_size;
+	uint64_t pc;
+	uint64_t attest_addr;
+	uint64_t attest_challenge[8];
+	uint64_t aux[16];
+	uint64_t enter_reason;
+	uint64_t psci_complete_result;
+	bool runnable;
+	bool emulatable_abort;
+	bool psci_pending;
+	bool state_running;
+	bool attest_in_progress;
+} rmm_rec_t;
+
+/* RMI error codes. */
+#define RMI_SUCCESS              0
+#define RMI_ERROR_INPUT          1
+#define RMI_ERROR_MEMORY         2
+#define RMI_ERROR_ALIAS          3
+#define RMI_ERROR_IN_USE         4
+#define RMI_ERROR_REALM_STATE    5
+#define RMI_ERROR_OWNER          6
+#define RMI_ERROR_REC            7
+#define RMI_ERROR_RTT_WALK       8
+#define RMI_ERROR_RTT_ENTRY      9
+#define RMI_ERROR_NOT_SUPPORTED 10
+#define RMI_ERROR_INTERNAL      11
+
 /* RMI handled by TRP */
 #define RMI_FNUM_VERSION_REQ		U(0x150)
 
 #define RMI_FNUM_GRANULE_DELEGATE	U(0x151)
 #define RMI_FNUM_GRANULE_UNDELEGATE	U(0x152)
+#define RMI_FNUM_DATA_CREATE           U(0x153)
+#define RMI_FNUM_DATA_CREATE_UNKNOWN   U(0x154)
+#define RMI_FNUM_DATA_DESTROY          U(0x155)
+#define RMI_FNUM_DATA_DISPOSE          U(0x156)
+#define RMI_FNUM_REALM_ACTIVATE        U(0x157)
+#define RMI_FNUM_REALM_CREATE          U(0x158)
+#define RMI_FNUM_REALM_DESTROY         U(0x159)
+#define RMI_FNUM_REC_CREATE            U(0x15A)
+#define RMI_FNUM_REC_DESTROY           U(0x15B)
+#define RMI_FNUM_REC_ENTER             U(0x15C)
+#define RMI_FNUM_RTT_CREATE            U(0x15D)
+#define RMI_FNUM_RTT_DESTROY           U(0x15E)
+#define RMI_FNUM_RTT_MAP_UNPROTECTED   U(0x15F)
+#define RMI_FNUM_RTT_MAP_PROTECTED     U(0x160)
+#define RMI_FNUM_RTT_READ_ENTRY        U(0x161)
+#define RMI_FNUM_RTT_UNMAP_UNPROTECTED U(0x162)
+#define RMI_FNUM_RTT_UNMAP_PROTECTED   U(0x163)
+#define RMI_FNUM_PSCI_COMPLETE         U(0x164)
+#define RMI_FNUM_FEATURES              U(0x165)
+#define RMI_FNUM_RTT_FOLD              U(0x166)
+#define RMI_FNUM_REC_AUX_COUNT         U(0x167)
+#define RMI_FNUM_DATA_CREATE_LEVEL     U(0x168)
+#define RMI_FNUM_DATA_CREATE_UNKNOWN_LEVEL U(0x169)
+#define RMI_FNUM_DATA_DESTROY_LEVEL    U(0x16A)
 
-#define RMI_RMM_REQ_VERSION		RMM_FID(SMC_64, RMI_FNUM_VERSION_REQ)
-
-#define RMI_RMM_GRANULE_DELEGATE	RMM_FID(SMC_64, \
-						RMI_FNUM_GRANULE_DELEGATE)
-#define RMI_RMM_GRANULE_UNDELEGATE	RMM_FID(SMC_64, \
-						RMI_FNUM_GRANULE_UNDELEGATE)
+#define RMI_RMM_REQ_VERSION		        RMM_FID(SMC_64, RMI_FNUM_VERSION_REQ)
+#define RMI_RMM_GRANULE_DELEGATE	    RMM_FID(SMC_64, RMI_FNUM_GRANULE_DELEGATE)
+#define RMI_RMM_GRANULE_UNDELEGATE	    RMM_FID(SMC_64, RMI_FNUM_GRANULE_UNDELEGATE)
+#define RMI_RMM_DATA_CREATE             RMM_FID(SMC_64, RMI_FNUM_DATA_CREATE)
+#define RMI_RMM_DATA_CREATE_UNKNOWN     RMM_FID(SMC_64, RMI_FNUM_DATA_CREATE_UNKNOWN)
+#define RMI_RMM_DATA_DESTROY            RMM_FID(SMC_64, RMI_FNUM_DATA_DESTROY)
+#define RMI_RMM_DATA_DISPOSE            RMM_FID(SMC_64, RMI_FNUM_DATA_DISPOSE)
+#define RMI_RMM_REALM_ACTIVATE          RMM_FID(SMC_64, RMI_FNUM_REALM_ACTIVATE)
+#define RMI_RMM_REALM_CREATE            RMM_FID(SMC_64, RMI_FNUM_REALM_CREATE)
+#define RMI_RMM_REALM_DESTROY           RMM_FID(SMC_64, RMI_FNUM_REALM_DESTROY)
+#define RMI_RMM_REC_CREATE              RMM_FID(SMC_64, RMI_FNUM_REC_CREATE)
+#define RMI_RMM_REC_DESTROY             RMM_FID(SMC_64, RMI_FNUM_REC_DESTROY)
+#define RMI_RMM_REC_ENTER               RMM_FID(SMC_64, RMI_FNUM_REC_ENTER)
+#define RMI_RMM_RTT_CREATE              RMM_FID(SMC_64, RMI_FNUM_RTT_CREATE)
+#define RMI_RMM_RTT_DESTROY             RMM_FID(SMC_64, RMI_FNUM_RTT_DESTROY)
+#define RMI_RMM_RTT_MAP_UNPROTECTED     RMM_FID(SMC_64, RMI_FNUM_RTT_MAP_UNPROTECTED)
+#define RMI_RMM_RTT_MAP_PROTECTED       RMM_FID(SMC_64, RMI_FNUM_RTT_MAP_PROTECTED)
+#define RMI_RMM_RTT_READ_ENTRY          RMM_FID(SMC_64, RMI_FNUM_RTT_READ_ENTRY)
+#define RMI_RMM_RTT_UNMAP_UNPROTECTED   RMM_FID(SMC_64, RMI_FNUM_RTT_UNMAP_UNPROTECTED)
+#define RMI_RMM_RTT_UNMAP_PROTECTED     RMM_FID(SMC_64, RMI_FNUM_RTT_UNMAP_PROTECTED)
+#define RMI_RMM_PSCI_COMPLETE           RMM_FID(SMC_64, RMI_FNUM_PSCI_COMPLETE)
+#define RMI_RMM_FEATURES                RMM_FID(SMC_64, RMI_FNUM_FEATURES)
+#define RMI_RMM_RTT_FOLD                RMM_FID(SMC_64, RMI_FNUM_RTT_FOLD)
+#define RMI_RMM_REC_AUX_COUNT           RMM_FID(SMC_64, RMI_FNUM_REC_AUX_COUNT)
+#define RMI_RMM_DATA_CREATE_LEVEL       RMM_FID(SMC_64, RMI_FNUM_DATA_CREATE_LEVEL)
+#define RMI_RMM_DATA_CREATE_UNKNOWN_LEVEL RMM_FID(SMC_64, RMI_FNUM_DATA_CREATE_UNKNOWN_LEVEL)
+#define RMI_RMM_DATA_DESTROY_LEVEL      RMM_FID(SMC_64, RMI_FNUM_DATA_DESTROY_LEVEL)
 
 /* Definitions for RMI VERSION */
 #define RMI_ABI_VERSION_MAJOR		U(0x0)
@@ -50,9 +214,96 @@ typedef struct trp_args {
 #define RMI_ABI_VERSION			((RMI_ABI_VERSION_MAJOR << 16) | \
 					RMI_ABI_VERSION_MINOR)
 
+/* an address is delegable iff it falls within the range of GPT and
+ * granule_state_table */
+static inline bool rmm_is_addr_delegable(uint64_t addr) {
+	return addr < (RMM_NUM_GRANULES << RMM_GRANULE_SIZE2);
+}
+static inline bool rmm_is_addr_granule_aligned(uint64_t addr) {
+	return (addr & 0xfff) == 0;
+}
+static inline bool rmm_is_addr_delegable_galigned(uint64_t addr) {
+	uint64_t mask = ~((RMM_NUM_GRANULES - 1) << RMM_GRANULE_SIZE2);
+	return (addr & mask) == 0;
+}
+
+/* The function rmm_is_addr_rtt_level_aligned() is used to evaluate whether
+ * an address is aligned to the address range described by an RTTE at a
+ * specified RTT level. */
+static inline bool rmm_is_addr_rtt_level_aligned(uint64_t addr, int level) {
+	uint64_t mask = (1 << (12 + 9 * (3 - level))) - 1;
+	return (addr & mask) == 0;
+}
+
+void rmm_granule_state_table_init (void);
+rmm_granule_state_e rmm_get_granule_state (uint64_t addr);
+void rmm_set_granule_state (uint64_t addr, rmm_granule_state_e new_state);
+static inline bool rmm_assert_granule_state (uint64_t addr, rmm_granule_state_e exp_state) {
+	return rmm_is_addr_delegable_galigned(addr) && rmm_get_granule_state(addr) == exp_state;
+}
+static inline bool rmm_assert_granule_state_level (uint64_t addr, int level, rmm_granule_state_e exp_state) {
+	if (!rmm_is_addr_rtt_level_aligned(addr, level))
+		return false;
+	const uint64_t size = RMM_GRANULE_SIZE << (9 * (3-level));
+	for (uint64_t offset = 0; offset < size; offset += RMM_GRANULE_SIZE) {
+		if (!rmm_assert_granule_state(addr + offset, exp_state))
+			return false;
+	}
+	return true;
+}
+
+/* Current RTT implementation assume 48-bit output address. I.e. bit[47:12] is OA.
+ * Bit 56 in block and page descriptors flags DESTROYED status. 0:UNASSIGNED; 1:DESTROYED
+ * Bit 55 in block and page descriptors flags NS/Realm. 0:Realm; 1:NS */
+#define RTT_OABIT_MIN 12
+#define RTT_OABIT_MAX 48 // well, it's max+1
+#define RTT_OA_MASK (((1UL << (RTT_OABIT_MAX - RTT_OABIT_MIN)) - 1) << RTT_OABIT_MIN)
+#define RTT_DESTROYED_BIT 56
+#define RTT_NS_BIT 55
+
+typedef struct rmm_rtt_walk_result {
+	uint64_t rtt_addr; /* The address of the RTT granule */
+	uint64_t *rtte; /* The address of the RTT entry. So rtt_addr <= rtte < rtt_addr+4K */
+	int level; /* Level of the RTT. rd.rtt_level_start <= level <= 3 */
+} rmm_rtt_walk_result_t;
+
+rmm_rtt_entry_state_e rmm_get_rtte_state (const rmm_rtt_walk_result_t *walk_result);
+uint64_t rmm_get_rtte_outaddr (const rmm_rtt_walk_result_t *walk_result);
+void rmm_set_rtte (rmm_rtt_walk_result_t *walk_result, uint64_t out_addr, bool valid_bit);
+void rmm_set_rtte_table (rmm_rtt_walk_result_t *walk_result, uint64_t child_rtt);
+/* change valid bit without affecting out addr, used by MAP_PROT and UNMAP_PROT */
+void rmm_set_rtte_valid (rmm_rtt_walk_result_t *walk_result, bool is_valid);
+/* soly used by RMI_DATA_DESTROY */
+void rmm_set_rtte_destroyed (rmm_rtt_walk_result_t *walk_result);
+/* soly used by RMI_MAP_UNPROTECTED */
+void rmm_set_rtte_ns (rmm_rtt_walk_result_t *walk_result, uint64_t ns_rtte);
+void rmm_rtt_walk (rmm_rtt_walk_result_t *result, const rmm_realm_t *rd, uint64_t target_addr, int target_level);
+void rmm_rtt_walk_close (rmm_rtt_walk_result_t *result);
+bool rmm_rtt_fold(rmm_rtt_walk_result_t *parent_rtte);
+
+static inline void rmm_remap_granule(uint64_t addr, unsigned int attr)
+{
+	rmm_remap_xlat_table_entry(addr, attr);
+}
+
+static inline void rmm_undo_remap_granule(uint64_t addr)
+{
+	rmm_undo_remap_xlat_table_entry(addr);
+}
+
+/* copy <size> bytes of memory from <ns_src> to <rmm_dst>
+ * <rmm_dst> must point to realm memory. I.e. RMM stack/heap, DELEGATED, RD, DATA...
+ * <ns_src> must point to non-secure memory.
+ * return true if copy is successful.
+ * return false if copy is failed, because for example ns_src points to ROOT. */
+bool rmm_read_ns(void *rmm_dst, const void *ns_src, size_t size);
+bool rmm_write_ns (void *ns_dst, const void *rmm_src, size_t size);
+
 /* Helper to issue SMC calls to BL31 */
 uint64_t trp_smc(trp_args_t *);
 
+uint64_t __realm_enter(rmm_rec_t *prec);
+
 /* The main function to executed only by Primary CPU */
 void trp_main(void);
 
diff --git a/services/std_svc/rmmd/trp/vgic-v3.c b/services/std_svc/rmmd/trp/vgic-v3.c
new file mode 100644
index 000000000..98825036f
--- /dev/null
+++ b/services/std_svc/rmmd/trp/vgic-v3.c
@@ -0,0 +1,290 @@
+#include <arch.h>
+#include "trp_private.h"
+#include "vgic-v3.h"
+
+static void rmm_gic_v3_set_lr(uint64_t val, int lr)
+{
+	switch (lr & 0xf) {
+	case 0:
+		write_ich_lr0_el2(val);
+		break;
+	case 1:
+		write_ich_lr1_el2(val);
+		break;
+	case 2:
+		write_ich_lr2_el2(val);
+		break;
+	case 3:
+		write_ich_lr3_el2(val);
+		break;
+	case 4:
+		write_ich_lr4_el2(val);
+		break;
+	case 5:
+		write_ich_lr5_el2(val);
+		break;
+	case 6:
+		write_ich_lr6_el2(val);
+		break;
+	case 7:
+		write_ich_lr7_el2(val);
+		break;
+	case 8:
+		write_ich_lr8_el2(val);
+		break;
+	case 9:
+		write_ich_lr9_el2(val);
+		break;
+	case 10:
+		write_ich_lr10_el2(val);
+		break;
+	case 11:
+		write_ich_lr11_el2(val);
+		break;
+	case 12:
+		write_ich_lr12_el2(val);
+		break;
+	case 13:
+		write_ich_lr13_el2(val);
+		break;
+	case 14:
+		write_ich_lr14_el2(val);
+		break;
+	case 15:
+		write_ich_lr15_el2(val);
+		break;
+	}
+}
+
+static uint64_t rmm_gic_v3_get_lr(unsigned int lr)
+{
+	switch (lr & 0xf) {
+	case 0:
+		return read_ich_lr0_el2();
+	case 1:
+		return read_ich_lr1_el2();
+	case 2:
+		return read_ich_lr2_el2();
+	case 3:
+		return read_ich_lr3_el2();
+	case 4:
+		return read_ich_lr4_el2();
+	case 5:
+		return read_ich_lr5_el2();
+	case 6:
+		return read_ich_lr6_el2();
+	case 7:
+		return read_ich_lr7_el2();
+	case 8:
+		return read_ich_lr8_el2();
+	case 9:
+		return read_ich_lr9_el2();
+	case 10:
+		return read_ich_lr10_el2();
+	case 11:
+		return read_ich_lr11_el2();
+	case 12:
+		return read_ich_lr12_el2();
+	case 13:
+		return read_ich_lr13_el2();
+	case 14:
+		return read_ich_lr14_el2();
+	case 15:
+		return read_ich_lr15_el2();
+	}
+
+	return 0;
+}
+
+static uint32_t rmm_vgic_v3_read_ap0rn(int n)
+{
+	uint32_t val;
+
+	switch (n) {
+	case 0:
+		val = read_ich_ap0r0_el2();
+		break;
+	case 1:
+		val = read_ich_ap0r1_el2();
+		break;
+	case 2:
+		val = read_ich_ap0r2_el2();
+		break;
+	case 3:
+		val = read_ich_ap0r3_el2();
+		break;
+	default:
+		__builtin_unreachable();
+	}
+
+	return val;
+}
+
+static uint32_t rmm_vgic_v3_read_ap1rn(int n)
+{
+	uint32_t val;
+
+	switch (n) {
+	case 0:
+		val = read_ich_ap1r0_el2();
+		break;
+	case 1:
+		val = read_ich_ap1r1_el2();
+		break;
+	case 2:
+		val = read_ich_ap1r2_el2();
+		break;
+	case 3:
+		val = read_ich_ap1r3_el2();
+		break;
+	default:
+		__builtin_unreachable();
+	}
+
+	return val;
+}
+
+static void rmm_vgic_v3_write_ap0rn(uint32_t val, int n)
+{
+	switch (n) {
+	case 0:
+		write_ich_ap0r0_el2(val);
+		break;
+	case 1:
+		write_ich_ap0r1_el2(val);
+		break;
+	case 2:
+		write_ich_ap0r2_el2(val);
+		break;
+	case 3:
+		write_ich_ap0r3_el2(val);
+		break;
+	default:
+		__builtin_unreachable();
+	}
+}
+
+static void rmm_vgic_v3_write_ap1rn(uint32_t val, int n)
+{
+	switch (n) {
+	case 0:
+		write_ich_ap1r0_el2(val);
+		break;
+	case 1:
+		write_ich_ap1r1_el2(val);
+		break;
+	case 2:
+		write_ich_ap1r2_el2(val);
+		break;
+	case 3:
+		write_ich_ap1r3_el2(val);
+		break;
+	default:
+		__builtin_unreachable();
+	}
+}
+
+static void rmm_vgic_v3_save_aprs(vgic_v3_cpu_if_t *cpu_if)
+{
+	uint64_t val;
+	uint32_t nr_pre_bits;
+
+	val = read_ich_vtr_el2();
+	nr_pre_bits = vtr_to_nr_pre_bits(val);
+
+	switch (nr_pre_bits) {
+	case 7:
+		cpu_if->vgic_ap0r[3] = rmm_vgic_v3_read_ap0rn(3);
+		cpu_if->vgic_ap0r[2] = rmm_vgic_v3_read_ap0rn(2);
+		__attribute__((__fallthrough__));
+	case 6:
+		cpu_if->vgic_ap0r[1] = rmm_vgic_v3_read_ap0rn(1);
+		__attribute__((__fallthrough__));
+	default:
+		cpu_if->vgic_ap0r[0] = rmm_vgic_v3_read_ap0rn(0);
+	}
+
+	switch (nr_pre_bits) {
+	case 7:
+		cpu_if->vgic_ap1r[3] = rmm_vgic_v3_read_ap1rn(3);
+		cpu_if->vgic_ap1r[2] = rmm_vgic_v3_read_ap1rn(2);
+		__attribute__((__fallthrough__));
+	case 6:
+		cpu_if->vgic_ap1r[1] = rmm_vgic_v3_read_ap1rn(1);
+		__attribute__((__fallthrough__));
+	default:
+		cpu_if->vgic_ap1r[0] = rmm_vgic_v3_read_ap1rn(0);
+	}
+}
+
+
+static void rmm_vgic_v3_restore_aprs(vgic_v3_cpu_if_t *cpu_if)
+{
+	uint64_t val;
+	uint32_t nr_pre_bits;
+
+	val = read_ich_vtr_el2();
+	nr_pre_bits = vtr_to_nr_pre_bits(val);
+
+	switch (nr_pre_bits) {
+	case 7:
+		rmm_vgic_v3_write_ap0rn(cpu_if->vgic_ap0r[3], 3);
+		rmm_vgic_v3_write_ap0rn(cpu_if->vgic_ap0r[2], 2);
+		__attribute__((__fallthrough__));
+	case 6:
+		rmm_vgic_v3_write_ap0rn(cpu_if->vgic_ap0r[1], 1);
+		__attribute__((__fallthrough__));
+	default:
+		rmm_vgic_v3_write_ap0rn(cpu_if->vgic_ap0r[0], 0);
+	}
+
+	switch (nr_pre_bits) {
+	case 7:
+		rmm_vgic_v3_write_ap1rn(cpu_if->vgic_ap1r[3], 3);
+		rmm_vgic_v3_write_ap1rn(cpu_if->vgic_ap1r[2], 2);
+		__attribute__((__fallthrough__));
+	case 6:
+		rmm_vgic_v3_write_ap1rn(cpu_if->vgic_ap1r[1], 1);
+		__attribute__((__fallthrough__));
+	default:
+		rmm_vgic_v3_write_ap1rn(cpu_if->vgic_ap1r[0], 0);
+	}
+}
+
+void rmm_vgic_v3_save_state(vgic_v3_cpu_if_t *cpu_if)
+{
+	unsigned int nr_lrs = (read_ich_vtr_el2() & 0xf) + 1;
+	unsigned int i;
+	uint32_t elrsr = read_ich_elrsr_el2();
+
+	cpu_if->vgic_vmcr = read_ich_vmcr_el2();
+	/* cpu_if->vgic_misr = read_ich_misr_el2(); */
+	cpu_if->vgic_hcr = read_ich_hcr_el2();
+
+	write_ich_hcr_el2(cpu_if->vgic_hcr & ~ICH_HCR_EN);
+	for (i = 0; i < nr_lrs; i++) {
+		if (elrsr & (1 << i))
+			cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		else
+			cpu_if->vgic_lr[i] = rmm_gic_v3_get_lr(i);
+
+		rmm_gic_v3_set_lr(0, i);
+	}
+
+	rmm_vgic_v3_save_aprs(cpu_if);
+}
+
+void rmm_vgic_v3_restore_state(vgic_v3_cpu_if_t *cpu_if)
+{
+	unsigned int nr_lrs = (read_ich_vtr_el2() & 0xf) + 1;
+	unsigned int i;
+
+	write_ich_hcr_el2(cpu_if->vgic_hcr);
+
+	for (i = 0; i < nr_lrs; i++) {
+		rmm_gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	}
+
+	write_ich_vmcr_el2(cpu_if->vgic_vmcr);
+	rmm_vgic_v3_restore_aprs(cpu_if);
+}
diff --git a/services/std_svc/rmmd/trp/vgic-v3.h b/services/std_svc/rmmd/trp/vgic-v3.h
new file mode 100644
index 000000000..8e0105d91
--- /dev/null
+++ b/services/std_svc/rmmd/trp/vgic-v3.h
@@ -0,0 +1,62 @@
+#ifndef VGIC_V3_H
+#define VGIC_V3_H
+#include <stdint.h>
+#include <assert.h>
+
+/*
+ * rmm vgic only support gicv3.
+ */
+
+#define ICH_HCR_EN          (1ULL << 0)
+#define ICH_HCR_UIE         (1ULL << 1)
+#define ICH_HCR_LRENPIE     (1ULL << 2)
+#define ICH_HCR_NPIE        (1ULL << 3)
+#define ICH_HCR_VGRP0EIE    (1ULL << 4)
+#define ICH_HCR_VGRP0DIE    (1ULL << 5)
+#define ICH_HCR_VGRP1EIE    (1ULL << 6)
+#define ICH_HCR_VGRP1DIE    (1ULL << 7)
+#define ICH_HCR_TC          (1ULL << 10)
+#define ICH_HCR_TALL0       (1ULL << 11)
+#define ICH_HCR_TALL1       (1ULL << 12)
+#define ICH_HCR_TDIR        (1ULL << 14)
+#define ICH_HCR_EOIcount_SHIFT  27
+#define ICH_HCR_EOIcount_MASK   (0x1fULL << ICH_HCR_EOIcount_SHIFT)
+
+#define REC_ENTRY_ICH_HCR_VALID_MASK \
+	(ICH_HCR_TDIR | ICH_HCR_VGRP0DIE | \
+	 ICH_HCR_VGRP0EIE |ICH_HCR_VGRP1DIE | \
+	 ICH_HCR_VGRP1EIE | ICH_HCR_NPIE | \
+	 ICH_HCR_LRENPIE | ICH_HCR_UIE)
+#define REC_EXIT_ICH_HCR_VALID_MASK \
+	(ICH_HCR_TDIR | ICH_HCR_VGRP0DIE | \
+	 ICH_HCR_VGRP0EIE |ICH_HCR_VGRP1DIE | \
+	 ICH_HCR_VGRP1EIE | ICH_HCR_NPIE | \
+	 ICH_HCR_LRENPIE | ICH_HCR_UIE | ICH_HCR_EOIcount_MASK)
+
+#define ICH_LR_EOI      (1ULL << 41)
+#define ICH_LR_GROUP    (1ULL << 60)
+#define ICH_LR_HW       (1ULL << 61)
+#define ICH_LR_STATE    (3ULL << 62)
+#define ICH_LR_PENDING_BIT  (1ULL << 62)
+#define ICH_LR_ACTIVE_BIT   (1ULL << 63)
+#define ICH_LR_PHYS_ID_SHIFT    32
+#define ICH_LR_PHYS_ID_MASK     (0x3ffULL << ICH_LR_PHYS_ID_SHIFT)
+#define ICH_LR_PRIOPITY_SHIFT   48
+#define ICH_LR_PRIOPITY_MASK    (0xffULL << ICH_LR_PRIOPITY_SHIFT)
+
+#define vtr_to_max_lr_idx(v)    ((v) & 0xf)
+#define vtr_to_nr_pre_bits(v)   ((((uint32_t)(v) >> 26) & 7) + 1)
+#define vtr_to_nr_apr_regs(v)   (1 << (vtr_to_nr_pre_bits(v) - 5))
+
+typedef struct vgic_v3_cpu_if {
+	uint32_t vgic_hcr;
+	uint32_t vgic_vmcr;
+	uint32_t vgic_ap0r[4];
+	uint32_t vgic_ap1r[4];
+	uint64_t vgic_lr[16];
+} vgic_v3_cpu_if_t;
+
+void rmm_vgic_v3_save_state(vgic_v3_cpu_if_t *cpu_if);
+void rmm_vgic_v3_restore_state(vgic_v3_cpu_if_t *cpu_if);
+
+#endif
